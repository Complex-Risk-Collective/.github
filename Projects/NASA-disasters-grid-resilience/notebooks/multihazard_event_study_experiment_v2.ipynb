{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef95525",
   "metadata": {},
   "source": [
    "This script is an experiment in analyzing multi-hazard impacts on the power grid\n",
    "\n",
    "The goal of this script is to identify a candidate set of wildfire + space weather events\n",
    "\n",
    "Approach (implemented)\n",
    "- Get wildfire point \n",
    "- Choose radius of some sort\n",
    "- Get any power grid disturbance that occurred within that time and circle\n",
    "- Inspect space weather data for preceding day to current time\n",
    "- Store full multi-hazard data for the candidate event and study forensically\n",
    "\n",
    "Possibility of a different approach (not implemented):\n",
    "- Read in set of wildfire events from MODIS or Suomi\n",
    "- Get time period, get spatial extent (GIS) -> visualize it\n",
    "- Highest level: get space weather indices\n",
    "    - more granular is to get magnetometers and GNSS data (GIMs, likely) in the area of the fire \n",
    "- Get HIFLD grid affected based on the wildfire\n",
    "- (optional) get observed power grid disturbances that overlap in space and time\n",
    "- Record the candidate event for forensic analysis (social media, interviews, grey literature exploration)\n",
    "\n",
    "\n",
    "TODO\n",
    "- the connection may very well come from space weather effects on communications during wildfire events (need to incorporate radiation data to represent space weather) \n",
    "- explore emdat database\n",
    "- add sophistication to wildfire region processing\n",
    "- add terrestrial weather to search\n",
    "- explore 2024 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b17e7d",
   "metadata": {},
   "source": [
    "#### Dependencies and Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77097901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "from datetime import datetime, time\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "# import contextily as ctx\n",
    "\n",
    "\n",
    "from supermag_api.supermag_api import *\n",
    "\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9447d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sm_one_minute(start,end):\n",
    "\n",
    "    '''\n",
    "\n",
    "    \n",
    "    Dependencies\n",
    "        supermag-api downloadable from https://supermag.jhuapl.edu/mag/?fidelity=low&start=2001-01-01T00%3A00%3A00.000Z&interval=1%3A00%3A00&tab=api\n",
    "            --> one should move this to the working directory and rename to supermag_api\n",
    "        datetime\n",
    "            \n",
    "    Notes\n",
    "        it appears this will only work for a few-day request; for longer periods, need to download the data directly\n",
    "            \n",
    "    '''\n",
    "\n",
    "\n",
    "#     import supermag_api\n",
    "#     import datetime\n",
    "\n",
    "    start_datetime = pd.to_datetime(start)\n",
    "    end_datetime = pd.to_datetime(end)\n",
    "    \n",
    "    # get the number of days between start and end\n",
    "    num_days = (end_datetime - start_datetime).days\n",
    "    print('the number of days that will be requested is {}'.format(num_days))\n",
    "    input('Press Enter if you are sure you want to ping the SuperMAG API for this...')\n",
    "    \n",
    "    # get correct format for 'start date'\n",
    "    # date_obj_start = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n",
    "    # formatted_start = date_obj_start.strftime('%Y-%m-%dT%H:%M')\n",
    "    date_obj_start = datetime.datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n",
    "    formatted_start = date_obj_start.strftime('%Y-%m-%dT%H:%M')\n",
    "\n",
    "    (status,df_smidxs) = SuperMAGGetIndices('rymc1012',formatted_start,86400*num_days,'SME, SMU, SML, SMR')\n",
    "\n",
    "    datetimes_sm = [pd.to_datetime(t, unit='s') for t in df_smidxs['tval']]\n",
    "\n",
    "    df_smidxs['datetimes'] = datetimes_sm\n",
    "    df_smidxs = df_smidxs.set_index(['datetimes'])\n",
    "    df_smidxs.index = pd.to_datetime(df_smidxs.index)\n",
    "    \n",
    "    return df_smidxs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a509d0",
   "metadata": {},
   "source": [
    "#### Wildfire database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d970b9",
   "metadata": {},
   "source": [
    "Some things I'm learning about wildfire data\n",
    "- MODIS might be best available given coverage\n",
    "- There is a NASA digital twin project https://ideas-digitaltwin.jpl.nasa.gov/airquality/, but it is focused on air quality and thus the atmospheric emissions associated with wildfire, whereas for this work I need the wildfires themselves (not their emissions which then mix with the atmosphere and its dynamics)\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c59c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASA FIRMS data https://firms.modaps.eosdis.nasa.gov\n",
    "\n",
    "\n",
    "# National Interagency Fire Center https://www.nifc.gov/\n",
    "file_path_nifc_wildfire = \"/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/wildfire_data/WFIGS_Incident_Locations_6818922478522849878.geojson\"\n",
    "file_path_modis_wildfire = \"/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/wildfire_data/DL_FIRE_M-C61_530609/fire_archive_M-C61_530609.shp\"\n",
    "# file_path_suomi_wildfire = \"/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/wildfire_data/DL_FIRE_SV-C2_530613/fire_archive_SV-C2_530613.shp\"\n",
    "\n",
    "# gdf_nifc_wildfire = gpd.read_file(file_path_nifc_wildfire)\n",
    "gdf_modis_wildfire = gpd.read_file(file_path_modis_wildfire)\n",
    "# gdf_suomi_wildfire = gpd.read_file(file_path_suomi_wildfire)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81dbdc-0fc5-435d-add1-cac064bba8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of the wildfire data is: {:.2f} MB'.format(sys.getsizeof(gdf_modis_wildfire) / 10**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the first few rows\n",
    "print(gdf_modis_wildfire.head())\n",
    "\n",
    "# Check the geometry type (e.g., Point, Polygon, LineString)\n",
    "print(gdf_modis_wildfire.geom_type.unique())\n",
    "\n",
    "# Get summary of the attributes\n",
    "print(gdf_modis_wildfire.info())\n",
    "\n",
    "# # Plot the shapefile (quick visualization)\n",
    "# gdf.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ACQ_DATE' in gdf_modis_wildfire.columns and 'ACQ_TIME' in gdf_modis_wildfire.columns:\n",
    "    gdf_modis_wildfire['datetime'] = (\n",
    "        gdf_modis_wildfire['ACQ_DATE'].astype(str) + ' ' + gdf_modis_wildfire['ACQ_TIME'].astype(str)\n",
    "    )\n",
    "    gdf_modis_wildfire['datetime'] = pd.to_datetime(gdf_modis_wildfire['datetime'])\n",
    "else:\n",
    "    raise ValueError(\"Expected 'ACQ_DATE' and 'ACQ_TIME' columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f09dc17",
   "metadata": {},
   "source": [
    "### (optional) visualize the wildfire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e282fc-6488-47fc-ab6d-94f5239c60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Convert GeoDataFrame to GeoJSON-like dictionary with timestamp for Folium\n",
    "features = []\n",
    "for _, row in gdf_modis_wildfire.iterrows():\n",
    "    feature = {\n",
    "        'type': 'Feature',\n",
    "        'geometry': row['geometry'].__geo_interface__,\n",
    "        'properties': {\n",
    "            'time': row['datetime'].isoformat(),  # Use ISO format for timestamps\n",
    "            'popup': f\"Acquired: {row['datetime']}\"\n",
    "        }\n",
    "    }\n",
    "    features.append(feature)\n",
    "\n",
    "# Create the Timestamped GeoJSON layer\n",
    "timestamped_geojson = TimestampedGeoJson(\n",
    "    {'type': 'FeatureCollection', 'features': features},\n",
    "    period='PT1H',  # Period for animation: 1 hour\n",
    "    add_last_point=False,\n",
    "    auto_play=False,\n",
    "    loop=True,\n",
    "    max_speed=1,\n",
    "    loop_button=True,\n",
    "    date_options='YYYY-MM-DD HH:mm:ss',\n",
    "    time_slider_drag_update=True\n",
    ")\n",
    "\n",
    "# Initialize a Folium map centered on the dataset's centroid\n",
    "m = folium.Map(\n",
    "    location=[gdf_modis_wildfire.geometry.y.mean(), gdf_modis_wildfire.geometry.x.mean()],\n",
    "    zoom_start=6, tiles='CartoDB positron'\n",
    ")\n",
    "\n",
    "# Add the timestamped layer to the map\n",
    "timestamped_geojson.add_to(m)\n",
    "\n",
    "# Save the map as an HTML file and display it\n",
    "m.save('interactive_map.html')\n",
    "\n",
    "# If running in Jupyter, display the map directly\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aaacca",
   "metadata": {},
   "source": [
    "### Create a list of features grouped by unique timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for dt, group in gdf_modis_wildfire.groupby('datetime'):\n",
    "    feature = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': [\n",
    "            {\n",
    "                'type': 'Feature',\n",
    "                'geometry': row['geometry'].__geo_interface__,\n",
    "                'properties': {\n",
    "                    'time': dt.isoformat(),  # Set timestamp in ISO format\n",
    "                    'popup': f\"Acquired: {dt}\"\n",
    "                }\n",
    "            }\n",
    "            for _, row in group.iterrows()\n",
    "        ]\n",
    "    }\n",
    "    features.append(feature)\n",
    "\n",
    "\n",
    "# # Create the TimestampedGeoJson with these features\n",
    "# timestamped_geojson = TimestampedGeoJson(\n",
    "#     {'type': 'FeatureCollection', 'features': features},\n",
    "#     period='PT1H',              # Set period of time for each step (1 hour)\n",
    "#     add_last_point=False,        # Ensure only current points appear\n",
    "#     auto_play=False,              # Automatically start animation\n",
    "#     loop=True,                   # Loop the animation\n",
    "#     max_speed=1,                 # Normal animation speed\n",
    "#     loop_button=True,            # Allow user to loop animation\n",
    "#     date_options='YYYY-MM-DD HH:mm:ss',  # Display format for timestamps\n",
    "#     time_slider_drag_update=True # Update on dragging time slider\n",
    "# )\n",
    "\n",
    "# # Initialize the Folium map centered on the dataset's centroid\n",
    "# m = folium.Map(\n",
    "#     location=[gdf_modis_wildfire.geometry.y.mean(), gdf_modis_wildfire.geometry.x.mean()],\n",
    "#     zoom_start=6, \n",
    "#     tiles='CartoDB positron'  # Light basemap\n",
    "# )\n",
    "\n",
    "# # Add the timestamped GeoJSON to the map\n",
    "# timestamped_geojson.add_to(m)\n",
    "\n",
    "# # Save the map as an HTML file and display it\n",
    "# m.save('interactive_map.html')\n",
    "\n",
    "# # If running in Jupyter, display the map directly\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2063916a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "849a9b35",
   "metadata": {},
   "source": [
    "#### Optionally read in the HIFLD data for power grid transmission lines\n",
    "uses HIFLD data: https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::transmission-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_powergrid = \"/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/physical_grid_data/U.S._Electric_Power_Transmission_Lines.geojson\"\n",
    "gdf_powergrid = gpd.read_file(file_path_powergrid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff248a-b2eb-46a0-8760-e60135d01090",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of the power grid data is: {:.2f} MB'.format(sys.getsizeof(gdf_powergrid) / 10**6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5e07d",
   "metadata": {},
   "source": [
    "#### Power Grid Disturbance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d68cc2",
   "metadata": {},
   "source": [
    "Candidates\n",
    "- [new dataset from EAGLE-I](https://www.nature.com/articles/s41597-024-03095-5) (Data are available in the Figshare repository at https://doi.org/10.6084/m9.figshare.24237376 or via archive request from the eagle-I website)\n",
    "- [DOE electric disturbance events OE-417](https://www.oe.netl.doe.gov/OE417_annual_summary.aspx)\n",
    "\n",
    "\n",
    "New Candidate: [NASA Black Marble](https://blackmarble.gsfc.nasa.gov/) (will require more thinking about how we might use these data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in 2023 data\n",
    "outage_directory = '/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/outage_data/'\n",
    "oe417 = pd.read_excel(os.path.join(outage_directory,'DOE-OE-417/2023_Annual_Summary.xlsx'),header=1)\n",
    "oe417 = oe417.dropna(subset=['Time Event Began'])\n",
    "oe417 = oe417.iloc[:-2]\n",
    "for o in range(len(oe417['Number of Customers Affected'])):\n",
    "    if oe417['Number of Customers Affected'].iloc[o]=='Unknown':\n",
    "        oe417['Number of Customers Affected'].iloc[o] = np.nan\n",
    "    elif type(oe417['Number of Customers Affected'].iloc[o]) == str: \n",
    "        oe417['Number of Customers Affected'].iloc[o] = int(oe417['Number of Customers Affected'].iloc[o])\n",
    "    \n",
    "oe417 = oe417[oe417['Number of Customers Affected']>1000]\n",
    "\n",
    "eaglei = pd.read_csv(os.path.join(outage_directory,'EAGLE-I/eaglei_outages_2023.csv'))\n",
    "# limit the data to >1000 customers out\n",
    "eaglei = eaglei[eaglei['customers_out']>1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b71b26-a903-4c18-b352-f91ee59dcce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of the oe417 data is: {:.2f} MB'.format(sys.getsizeof(oe417) / 10**6))\n",
    "print('size of the eaglei data is: {:.2f} MB'.format(sys.getsizeof(eaglei) / 10**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b24d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oe417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def get_datetime_oe417(date,time):\n",
    "    # try: \n",
    "    #     dt = datetime.combine(str_date,dt_time)\n",
    "    # except: \n",
    "    #     try:\n",
    "    #         date_obj = datetime.strptime(str_date, '%m/%d/%Y')\n",
    "    #         # dt = datetime.combine(date_obj, dt_time)\n",
    "    #         dt = parse(date_obj+' '+dt_time)\n",
    "    #     except:\n",
    "    #         dt = np.nan\n",
    "\n",
    "    if type(date) == str:\n",
    "        date_obj = datetime.strptime(date, '%m/%d/%Y')\n",
    "        dt = datetime.combine(date_obj,time)\n",
    "    else:\n",
    "        dt = datetime.combine(date,time)\n",
    "    \n",
    "    return dt\n",
    "\n",
    "# Regular expression to capture 'State: County' pairs\n",
    "pattern = r'(\\w+):\\s*([\\w\\s-]+)'\n",
    "\n",
    "# Function to extract states and counties from each row\n",
    "# def extract_state_county_pairs(row):\n",
    "#     pairs = []\n",
    "#     # Split by semicolon to handle multiple state-county pairs\n",
    "#     entries = row.split(';')\n",
    "#     for entry in entries:\n",
    "#         # Use regex to extract state and county\n",
    "#         match = re.match(r'(\\w+):\\s*(.*)', entry.strip())\n",
    "#         if match:\n",
    "#             state = match.group(1)\n",
    "#             county = match.group(2)\n",
    "#             pairs.append((state, county))\n",
    "#     return pairs\n",
    "\n",
    "def extract_state_county_pairs(location_str):\n",
    "    pairs = []\n",
    "\n",
    "    # Split by ';' to handle multiple state-county blocks\n",
    "    state_county_blocks = location_str.split(';')\n",
    "\n",
    "    for block in state_county_blocks:\n",
    "        block = block.strip()\n",
    "\n",
    "        # Case: Handle multiple consecutive states without counties (e.g., \"Alabama: Maine:\")\n",
    "        if re.fullmatch(r\"(.+?:)+\", block):\n",
    "            states = [state.strip() for state in block.split(':') if state.strip()]\n",
    "            for state in states:\n",
    "                pairs.append((state, None))  # Store each state with None for county\n",
    "            continue  # Skip further processing for this block\n",
    "\n",
    "        # Use regex to match \"State: County, County, ...\" or \"State:\"\n",
    "        match = re.match(r\"(.+?):\\s*(.*)\", block)\n",
    "\n",
    "        if match:\n",
    "            state = match.group(1).strip()\n",
    "            counties = match.group(2).strip()\n",
    "\n",
    "            if counties:  # If counties exist, split them by ','\n",
    "                counties_list = [county.strip() for county in counties.split(',')]\n",
    "                for county in counties_list:\n",
    "                    pairs.append((state, county))\n",
    "            else:  # If no counties are provided\n",
    "                pairs.append((state, None))\n",
    "        else:\n",
    "            # If the block contains only a state name\n",
    "            pairs.append((block, None))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def get_oe417_geometries(row_area_affected,gdf_states,gdf_counties):\n",
    "    '''\n",
    "        inputs: \n",
    "            - row of oe417 data for Area Affected (oe417['Area Affected'].iloc[x])\n",
    "            - data for states geometries\n",
    "            - data for county geometries\n",
    "        dependencies:\n",
    "            - extract_state_county_pairs function\n",
    "    '''\n",
    "    \n",
    "    row_states_counties = extract_state_county_pairs(row_area_affected)\n",
    "    all_geometries = []\n",
    "    for l in row_states_counties: \n",
    "        if not l[0]:\n",
    "            # skip the empty rows in the extracted data\n",
    "            continue\n",
    "        else:\n",
    "            if not l[1]:\n",
    "                # if there is no county data, then just use whole state\n",
    "                geom_data_tmp = gpd.GeoSeries(gdf_states['geometry'][gdf_states['NAME'] == l[0]].values)\n",
    "                if geom_data_tmp.empty:\n",
    "                    continue\n",
    "                if geom_data_tmp.apply(lambda geom: isinstance(geom, MultiPolygon)).all():\n",
    "#                     print('----> encountering multipolygon for {}, skipping'.format(l[0]))\n",
    "                    all_geometries.extend([polygon for polygon in geom_data_tmp.geometry[0].geoms])\n",
    "                else:\n",
    "                    all_geometries.extend( [Polygon(geom_data_tmp.geometry[0])] )\n",
    "            else:\n",
    "                # if there is county data, get the county geometries and combine them\n",
    "                fips_tmp = get_fips(l[0])\n",
    "                geom_data_tmp = gpd.GeoSeries(gdf_counties['geometry'][ (gdf_counties['STATEFP'] == fips_tmp) & (gdf_counties['NAMELSAD'] == l[1]) ].values)\n",
    "                if geom_data_tmp.empty:\n",
    "                    continue\n",
    "                if geom_data_tmp.apply(lambda geom: isinstance(geom, MultiPolygon)).all():\n",
    "#                     print('----> encountering multipolygon for {}, skipping'.format(l[0]))\n",
    "                    all_geometries.extend([polygon for polygon in geom_data_tmp.geometry[0].geoms])\n",
    "                else:\n",
    "                    all_geometries.extend( [Polygon(geom_data_tmp.geometry[0])] )\n",
    "\n",
    "    combined_geometry = gpd.GeoSeries(all_geometries).unary_union\n",
    "\n",
    "    return combined_geometry, row_states_counties\n",
    "\n",
    "def get_eaglei_geometries(row_county,row_state,gdf_states,gdf_counties):\n",
    "    '''\n",
    "        inputs: \n",
    "            - row of eaglei data for county (eagelei['county'].iloc[x])\n",
    "            - row of eaglei data for state (eagelei['state'].iloc[x])\n",
    "            - data for states geometries\n",
    "            - data for county geometries\n",
    "        dependencies:\n",
    "            - extract_state_county_pairs function\n",
    "    '''\n",
    "    \n",
    "    all_geometries = []\n",
    "    fips_tmp = get_fips(row_state)\n",
    "    geom_data_tmp = gpd.GeoSeries(gdf_counties['geometry'][ (gdf_counties['STATEFP'] == fips_tmp) & (gdf_counties['NAMELSAD'] == row_county + ' County') ].values)\n",
    "    if geom_data_tmp.empty:\n",
    "        return np.nan\n",
    "    if geom_data_tmp.apply(lambda geom: isinstance(geom, MultiPolygon)).all():\n",
    "        all_geometries.extend([polygon for polygon in geom_data_tmp.geometry[0].geoms])\n",
    "    else:\n",
    "        all_geometries.extend( [Polygon(geom_data_tmp.geometry[0])] )\n",
    "    \n",
    "\n",
    "    combined_geometry = gpd.GeoSeries(all_geometries).unary_union\n",
    "\n",
    "    return combined_geometry\n",
    "\n",
    "# Dictionary mapping state abbreviations to FIPS codes\n",
    "# List of tuples containing (state_abbreviation, fips_code, full_state_name)\n",
    "state_fips = [\n",
    "    ('AL', '01', 'Alabama'),\n",
    "    ('AK', '02', 'Alaska'),\n",
    "    ('AZ', '04', 'Arizona'),\n",
    "    ('AR', '05', 'Arkansas'),\n",
    "    ('CA', '06', 'California'),\n",
    "    ('CO', '08', 'Colorado'),\n",
    "    ('CT', '09', 'Connecticut'),\n",
    "    ('DE', '10', 'Delaware'),\n",
    "    ('FL', '12', 'Florida'),\n",
    "    ('GA', '13', 'Georgia'),\n",
    "    ('HI', '15', 'Hawaii'),\n",
    "    ('ID', '16', 'Idaho'),\n",
    "    ('IL', '17', 'Illinois'),\n",
    "    ('IN', '18', 'Indiana'),\n",
    "    ('IA', '19', 'Iowa'),\n",
    "    ('KS', '20', 'Kansas'),\n",
    "    ('KY', '21', 'Kentucky'),\n",
    "    ('LA', '22', 'Louisiana'),\n",
    "    ('ME', '23', 'Maine'),\n",
    "    ('MD', '24', 'Maryland'),\n",
    "    ('MA', '25', 'Massachusetts'),\n",
    "    ('MI', '26', 'Michigan'),\n",
    "    ('MN', '27', 'Minnesota'),\n",
    "    ('MS', '28', 'Mississippi'),\n",
    "    ('MO', '29', 'Missouri'),\n",
    "    ('MT', '30', 'Montana'),\n",
    "    ('NE', '31', 'Nebraska'),\n",
    "    ('NV', '32', 'Nevada'),\n",
    "    ('NH', '33', 'New Hampshire'),\n",
    "    ('NJ', '34', 'New Jersey'),\n",
    "    ('NM', '35', 'New Mexico'),\n",
    "    ('NY', '36', 'New York'),\n",
    "    ('NC', '37', 'North Carolina'),\n",
    "    ('ND', '38', 'North Dakota'),\n",
    "    ('OH', '39', 'Ohio'),\n",
    "    ('OK', '40', 'Oklahoma'),\n",
    "    ('OR', '41', 'Oregon'),\n",
    "    ('PA', '42', 'Pennsylvania'),\n",
    "    ('RI', '44', 'Rhode Island'),\n",
    "    ('SC', '45', 'South Carolina'),\n",
    "    ('SD', '46', 'South Dakota'),\n",
    "    ('TN', '47', 'Tennessee'),\n",
    "    ('TX', '48', 'Texas'),\n",
    "    ('UT', '49', 'Utah'),\n",
    "    ('VT', '50', 'Vermont'),\n",
    "    ('VA', '51', 'Virginia'),\n",
    "    ('WA', '53', 'Washington'),\n",
    "    ('WV', '54', 'West Virginia'),\n",
    "    ('WI', '55', 'Wisconsin'),\n",
    "    ('WY', '56', 'Wyoming'),\n",
    "]\n",
    "\n",
    "def get_fips(state_name):\n",
    "    for abbr, fips, name in state_fips:\n",
    "        if name.upper() == state_name.upper():\n",
    "            return fips\n",
    "    return None, None  # Return None if not found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3164cf2",
   "metadata": {},
   "source": [
    "##### Country/State location data\n",
    "Geting the data from [US Census Bureau Tiger platform](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75908b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_states = gpd.read_file('/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/location_data/Census_Bureau_Data/tl_2023_us_state/tl_2023_us_state.shp')\n",
    "gdf_counties = gpd.read_file('/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/location_data/Census_Bureau_Data/tl_2023_us_county/tl_2023_us_county.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195fd2d-c896-4c9f-aa11-5c4b08308c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of the states data is: {:.2f} MB'.format(sys.getsizeof(gdf_states) / 10**6))\n",
    "print('size of the counties data is: {:.2f} MB'.format(sys.getsizeof(gdf_counties) / 10**6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb18a2f-a3e1-4a59-85e8-57119759c8a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge into one outage data file as a gpd\n",
    "outage_columns = ['outage_start_time', 'outage_stop_time', 'customers_affected','geometry','source']\n",
    "outage_data_gdf = gpd.GeoDataFrame(pd.DataFrame(columns=outage_columns), geometry='geometry')\n",
    "\n",
    "outage_data_gdf.set_crs(\"EPSG:4326\", inplace=True)  # WGS84 - lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e33b1-602f-45d1-b4ff-405b8a3d601b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# growing the outage data with oe417 data\n",
    "\n",
    "for o in range(len(oe417)):\n",
    "    \n",
    "    # get start and end times\n",
    "    str_date_start = oe417['Date Event Began'].iloc[o]\n",
    "    dt_time_start = oe417['Time Event Began'].iloc[o]\n",
    "    str_date_end = oe417['Date of Restoration'].iloc[o]\n",
    "    dt_time_end = oe417['Time of Restoration'].iloc[o]\n",
    "    try:\n",
    "        dt_start = get_datetime_oe417(str_date_start,dt_time_start)\n",
    "    except:\n",
    "        dt_start = np.nan\n",
    "    try:\n",
    "        dt_end = get_datetime_oe417(str_date_end,dt_time_end)\n",
    "    except:\n",
    "        dt_end = np.nan    \n",
    "\n",
    "    # get the geometries of the areas affected\n",
    "    combined_geometry, states_counties = get_oe417_geometries(oe417['Area Affected'].iloc[o],gdf_states,gdf_counties)\n",
    "    \n",
    "    new_row = {\n",
    "        'source': 'oe417', \n",
    "        'area affected': states_counties, \n",
    "        'outage_start_time': dt_start,\n",
    "        'outage_stop_time': dt_end,\n",
    "        'customers_affected': oe417['Number of Customers Affected'].iloc[o],\n",
    "        'geometry': combined_geometry\n",
    "    }\n",
    "    \n",
    "    outage_data_gdf = pd.concat([outage_data_gdf, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e484e-e53b-4b00-982a-616a4d16cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outage_data_gdf#['outage_start_time'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd2bd8-1457-4193-a03c-1d130722dc4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# growing the outage data with eaglei data\n",
    "\n",
    "for e in range(len(eaglei)):\n",
    "    \n",
    "    # get start times\n",
    "    try:\n",
    "        dt_start = pd.to_datetime(eaglei['run_start_time'].iloc[e], format='%m/%d/%y %H:%M')\n",
    "    except:\n",
    "        dt_start = np.nan\n",
    "    \n",
    "    combined_geometry = get_eaglei_geometries(eaglei['county'].iloc[e],eaglei['state'].iloc[e],gdf_states,gdf_counties)\n",
    "    \n",
    "    outage_data_gdf = outage_data_gdf.append({\n",
    "                                                'source': 'eaglei', \n",
    "                                                'area affected': [(eaglei['state'].iloc[e],eaglei['county'].iloc[e]+' County')], \n",
    "                                                'outage_start_time': dt_start,\n",
    "                                                'outage_stop_time': np.nan,\n",
    "                                                'customers_affected': eaglei['customers_out'].iloc[e],\n",
    "                                                'geometry': combined_geometry\n",
    "                                                    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34084ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23979c96-2a20-44eb-8ad7-37b47e46c09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c60a2ee2",
   "metadata": {},
   "source": [
    "#### Candidate search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c35991",
   "metadata": {},
   "source": [
    "#### first group the modis wildfire data by datetime and nearby lat-long points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756113d-9c46-4964-8f45-0d7557af628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "\n",
    "def aggregate_points_within_radius(gdf, radius=0.01):\n",
    "    \"\"\"\n",
    "    Aggregates points by datetime and spatial proximity into polygons.\n",
    "\n",
    "    Parameters:\n",
    "    - df: GeoDataFrame with columns [\"datetime\", \"geometry\"]\n",
    "    - radius: Proximity radius in degrees (~0.01 degrees ≈ 1.1 km) for grouping\n",
    "    \n",
    "    Returns:\n",
    "    - A new GeoDataFrame with datetime and polygon geometry\n",
    "    \"\"\"\n",
    "    # Group by datetime\n",
    "    grouped = gdf.groupby('datetime')\n",
    "\n",
    "    result_rows = []\n",
    "\n",
    "    for datetime_value, group in grouped:\n",
    "        points = group['geometry'].tolist()\n",
    "        clustered_points = []\n",
    "\n",
    "        # Track visited points\n",
    "        visited = set()\n",
    "\n",
    "        for i, point in enumerate(points):\n",
    "            if i in visited:\n",
    "                continue\n",
    "\n",
    "            # Start a cluster\n",
    "            cluster = [point]\n",
    "            visited.add(i)\n",
    "\n",
    "            for j, other_point in enumerate(points):\n",
    "                if j not in visited and point.distance(other_point) <= radius:\n",
    "                    cluster.append(other_point)\n",
    "                    visited.add(j)\n",
    "\n",
    "            # Create a polygon from the clustered points\n",
    "            polygon = MultiPoint(cluster).convex_hull\n",
    "            result_rows.append({\n",
    "                'datetime': datetime_value,\n",
    "                'geometry': polygon\n",
    "            })\n",
    "\n",
    "    # Create a new GeoDataFrame from the result\n",
    "    result_gdf = gpd.GeoDataFrame(result_rows, columns=['datetime', 'geometry'], crs=gdf.crs)\n",
    "    return result_gdf\n",
    "\n",
    "\n",
    "# Example usage with your GeoDataFrame\n",
    "gdf_modis_wildfire_result = aggregate_points_within_radius(gdf_modis_wildfire, radius=0.01)\n",
    "print(gdf_modis_wildfire_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec10ab",
   "metadata": {},
   "source": [
    "##### Optionally plot grouped wildfire instances in MODIS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff8e86-eae1-42e8-896c-7981340f2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 0.1  # Example radius in degrees (adjust based on your CRS)\n",
    "\n",
    "candidates = []\n",
    "# candidate_columns = ['outage_start_time', 'outage_stop_time', 'customers_affected','geometry','source']\n",
    "# candidate_gdf = gpd.GeoDataFrame(pd.DataFrame(columns=outage_columns), geometry='geometry')\n",
    "\n",
    "\n",
    "# for w in range(len(gdf_modis_wildfire)):\n",
    "for index_w, row_w in gdf_modis_wildfire_result.iterrows():\n",
    "    \n",
    "    # Identify overlap in time with the power grid disturbance\n",
    "    wildfire_datetime = row_w.datetime #gdf_modis_wildfire['datetime'].iloc[w]\n",
    "    filtered_outage_data_gdf = outage_data_gdf[\n",
    "                                            (outage_data_gdf['outage_start_time'] >= wildfire_datetime - pd.Timedelta(hours=1)) &\n",
    "                                            (outage_data_gdf['outage_start_time'] <= wildfire_datetime + pd.Timedelta(days=1))\n",
    "                                            ]\n",
    "    \n",
    "    # Identify overlap in space with the power grid disturbance\n",
    "    geom = row_w.geometry\n",
    "    \n",
    "    # Check if the geometry is a Point\n",
    "    if geom.geom_type == 'Point':\n",
    "        #   Create a buffer (circular area) around the point\n",
    "        wildfire_point = gdf_modis_wildfire['geometry'].iloc[row_w.name]\n",
    "        buffered_wildfire_area = geom.buffer(radius)\n",
    "        disp_geom = geom.coords[0]\n",
    "\n",
    "    # Check if the geometry is a Polygon\n",
    "    elif geom.geom_type == 'Polygon':\n",
    "        buffered_wildfire_area = geom\n",
    "        disp_geom = geom.exterior.coords[0]\n",
    "\n",
    "    \n",
    "    for f in range(len(filtered_outage_data_gdf)):\n",
    "        if buffered_wildfire_area.intersects(filtered_outage_data_gdf['geometry'].iloc[f]):\n",
    "            \n",
    "            # Check space weather data for space weather conditions (if at any time in the surrounding 24 hours AE exceeded 200 nT)\n",
    "            sm_data = get_sm_one_minute((wildfire_datetime - pd.Timedelta(hours=24)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                                        (wildfire_datetime + pd.Timedelta(hours=12)).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            \n",
    "            if sm_data['SME'][sm_data['SME']>200.].any():\n",
    "\n",
    "\n",
    "                # save as a candidate\n",
    "                print('candidate at {}! \\n wildfire: \\n \\t {} \\n outage: \\n \\t {} for {} \\n sp wx: \\n \\t {} at {}'.format(wildfire_datetime,\n",
    "                                                                                                                             disp_geom,\n",
    "                                                                                                                             filtered_outage_data_gdf['outage_start_time'].iloc[f],\n",
    "                                                                                                                             filtered_outage_data_gdf['area affected'].iloc[f],\n",
    "                                                                                                                             sm_data['SME'][sm_data['SME']==sm_data['SME'].max()].values,\n",
    "                                                                                                                             sm_data.index[sm_data['SME']==sm_data['SME'].max()].values,))\n",
    "                candidates.append({\n",
    "                                    'wildfire datetime': wildfire_datetime,\n",
    "                                    'wildfire geometry': geom,\n",
    "                                    'outage_start_time': filtered_outage_data_gdf['outage_start_time'].iloc[f],\n",
    "                                    'outage_stop_time': filtered_outage_data_gdf['outage_stop_time'].iloc[f],\n",
    "                                    'customers affected': filtered_outage_data_gdf['customers_affected'].iloc[f],\n",
    "                                    'outage geometry': filtered_outage_data_gdf['geometry'].iloc[f],\n",
    "                                    'area affected': filtered_outage_data_gdf['area affected'].iloc[f],\n",
    "                                    'max SME': sm_data['SME'][sm_data['SME']==sm_data['SME'].max()].values,\n",
    "\n",
    "                                    })\n",
    "                break\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671b3bb2-c88d-4620-bd26-64eb0a2cf8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c883511",
   "metadata": {},
   "source": [
    "#### Candidate search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaea6bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "radius = 0.1  # Example radius in degrees (adjust based on your CRS)\n",
    "\n",
    "candidates = []\n",
    "# candidate_columns = ['outage_start_time', 'outage_stop_time', 'customers_affected','geometry','source']\n",
    "# candidate_gdf = gpd.GeoDataFrame(pd.DataFrame(columns=outage_columns), geometry='geometry')\n",
    "\n",
    "\n",
    "# for w in range(len(gdf_modis_wildfire)):\n",
    "for index_w, row_w in gdf_modis_wildfire_result.iterrows():\n",
    "    \n",
    "    # Identify overlap in time with the power grid disturbance\n",
    "    wildfire_datetime = row_w.datetime #gdf_modis_wildfire['datetime'].iloc[w]\n",
    "    filtered_outage_data_gdf = outage_data_gdf[\n",
    "                                            (outage_data_gdf['outage_start_time'] >= wildfire_datetime - pd.Timedelta(hours=1)) &\n",
    "                                            (outage_data_gdf['outage_start_time'] <= wildfire_datetime + pd.Timedelta(days=1))\n",
    "                                            ]\n",
    "    \n",
    "    # Identify overlap in space with the power grid disturbance\n",
    "    geom = row_w.geometry\n",
    "    \n",
    "    # Check if the geometry is a Point\n",
    "    if geom.geom_type == 'Point':\n",
    "        #   Create a buffer (circular area) around the point\n",
    "        wildfire_point = gdf_modis_wildfire['geometry'].iloc[row_w.name]\n",
    "        buffered_wildfire_area = geom.buffer(radius)\n",
    "        disp_geom = geom.coords[0]\n",
    "\n",
    "    # Check if the geometry is a Polygon\n",
    "    elif geom.geom_type == 'Polygon':\n",
    "        buffered_wildfire_area = geom\n",
    "        disp_geom = geom.exterior.coords[0]\n",
    "\n",
    "    \n",
    "    for f in range(len(filtered_outage_data_gdf)):\n",
    "        if buffered_wildfire_area.intersects(filtered_outage_data_gdf['geometry'].iloc[f]):\n",
    "            \n",
    "            # Check space weather data for space weather conditions (if at any time in the surrounding 24 hours AE exceeded 200 nT)\n",
    "            sm_data = get_sm_one_minute((wildfire_datetime - pd.Timedelta(hours=24)).strftime('%Y-%-m-%-d %H:%M:%S'),\n",
    "                                        (wildfire_datetime + pd.Timedelta(hours=12)).strftime('%Y-%-m-%-d %H:%M:%S'))\n",
    "            \n",
    "            if sm_data['SME'][sm_data['SME']>200.].any():\n",
    "\n",
    "\n",
    "                # save as a candidate\n",
    "                print('candidate at {}! \\n wildfire: \\n \\t {} \\n outage: \\n \\t {} for {} \\n sp wx: \\n \\t {} at {}'.format(wildfire_datetime,\n",
    "                                                                                                                             disp_geom,\n",
    "                                                                                                                             filtered_outage_data_gdf['outage_start_time'].iloc[f],\n",
    "                                                                                                                             filtered_outage_data_gdf['area affected'].iloc[f],\n",
    "                                                                                                                             sm_data['SME'][sm_data['SME']==sm_data['SME'].max()].values,\n",
    "                                                                                                                             sm_data.index[sm_data['SME']==sm_data['SME'].max()].values,))\n",
    "                candidates.append({\n",
    "                                    'wildfire datetime': wildfire_datetime,\n",
    "                                    'wildfire geometry': geom,\n",
    "                                    'outage_start_time': filtered_outage_data_gdf['outage_start_time'].iloc[f],\n",
    "                                    'outage_stop_time': filtered_outage_data_gdf['outage_stop_time'].iloc[f],\n",
    "                                    'customers affected': filtered_outage_data_gdf['customers_affected'].iloc[f],\n",
    "                                    'outage geometry': filtered_outage_data_gdf['geometry'].iloc[f],\n",
    "                                    'area affected': filtered_outage_data_gdf['area affected'].iloc[f],\n",
    "                                    'max SME': sm_data['SME'][sm_data['SME']==sm_data['SME'].max()].values,\n",
    "\n",
    "                                    })\n",
    "                break\n",
    "    continue\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70422f-55a6-45bf-a7aa-958a20225f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5698971-a0dc-4423-89fd-7236d5952657",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414c41e-40b5-4b27-a4d5-0e4253b80539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39c565-745a-448e-bd87-2e2807c91786",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_candidates = gpd.GeoDataFrame(candidates)\n",
    "gdf_candidates = gdf_candidates.set_geometry(\"outage geometry\")\n",
    "\n",
    "\n",
    "# Save with pickle \n",
    "gdf_candidates.to_pickle(\"/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/candidate_multihazards_data/wildfire_space-weather_2023_candidates.pkl\")\n",
    "# loaded_gdf = gpd.read_pickle(\"hazard_events.pkl\")\n",
    "\n",
    "# # Save to GeoJSON (need to ensure all geometry types are the same before this works)\n",
    "# gdf_candidates.to_file(\"/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/candidate_multihazards_data/wildfire_space-weather_2023_candidates.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93714b0-46a6-499f-be30-d975cbc2a5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b4dc9a6-0feb-45c8-85ce-697bec302703",
   "metadata": {},
   "source": [
    "#### Load EM-DAT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39aab3-0a3e-4820-88d3-a8c49fd743cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_emdat = '/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/emdat_data/'\n",
    "file_emdat = 'public_emdat_2023_US.xlsx'\n",
    "df_emdat = pd.read_excel(os.path.join(directory_emdat,file_emdat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f9165-34f3-4157-b28d-ad61f6662f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_emdat.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf016c-6c9f-4e7a-a2c4-d631d6a95481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_emdat[['Start Year', 'Start Month', 'Start Day']]\n",
    "df_emdat[['End Year', 'End Month', 'End Day']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eff5d6-f145-4637-9ab2-4f45a7b359de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3257b-f0c2-4a1f-b147-74f247689132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emdat['start_datetime'] = df_emdat.apply(\n",
    "    lambda row: pd.Timestamp(int(row['Start Year']), int(row['Start Month']), int(row['Start Day'])) \n",
    "    if pd.notna(row['Start Year']) and pd.notna(row['Start Month']) and pd.notna(row['Start Day'])\n",
    "    else pd.NaT,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_emdat['end_datetime'] = df_emdat.apply(\n",
    "    lambda row: pd.Timestamp(int(row['End Year']), int(row['End Month']), int(row['End Day'])) \n",
    "    if pd.notna(row['End Year']) and pd.notna(row['End Month']) and pd.notna(row['End Day'])\n",
    "    else pd.NaT,\n",
    "    axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc50fca-474c-4432-af89-036a85364842",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_candidates.columns.tolist() #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d2210-260a-416f-9dbd-c33704c4e963",
   "metadata": {},
   "source": [
    "#### Compare potential events across candidates and database(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcec637-a5d3-4a4c-8550-6a60ed433c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx,row_emdat in df_emdat.iterrows():\n",
    "    tmp_diff = (gdf_candidates['outage_start_time'] - row_emdat['start_datetime']).abs()\n",
    "    result = gdf_candidates[tmp_diff <= pd.Timedelta(days=1)]\n",
    "\n",
    "    print('overlap of time around {}, \\n \\t location candidate = {} \\n \\t  location emdat = {}'.format(row_emdat['start_datetime'],result['area affected'],row_emdat['Location']))\n",
    "    print('\\n\\n')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f7046-9152-4e4a-867d-052763bad00b",
   "metadata": {},
   "source": [
    "Check on\n",
    "\n",
    "\n",
    "overlap of time around 2023-04-01 00:00:00, \n",
    " \t location candidate = 10    [(Arkansas, None)]\n",
    "11    [(Illinois, None)]\n",
    "Name: area affected, dtype: object \n",
    " \t  location emdat = Texas, Louisiana, Oklahoma, Kansas, Illinois, Missouri, Nebraska, Washington, Oregon, Montana\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4347a6-65e9-4382-830d-b1fe5e8cfddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_candidates_multihazard = gdf_candidates[(gdf_candidates['outage_start_time'] >= datetime(2023,3,31)) & (gdf_candidates['outage_start_time'] <= datetime(2023,4,2))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76814c2-a48e-47c2-b181-6adf7e0fcc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import folium\n",
    "# import time\n",
    "# from shapely.geometry import Point, Polygon\n",
    "# from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "# Create a Folium map centered over CONUS\n",
    "conus_map = folium.Map(location=[37.5, -95], zoom_start=5)\n",
    "\n",
    "for index, row in gdf_candidates_multihazard.iterrows():\n",
    "    geom = row['wildfire geometry']\n",
    "\n",
    "    # Check if the geometry is a Point\n",
    "    if geom.geom_type == 'Point':\n",
    "        marker_layer = folium.Marker(\n",
    "            location=[geom.y, geom.x],\n",
    "            popup=\"Point Geometry\"\n",
    "        ).add_to(conus_map)\n",
    "    \n",
    "    # Check if the geometry is a Polygon\n",
    "    elif geom.geom_type == 'Polygon':\n",
    "        poly_layer = folium.Polygon(\n",
    "            locations=[(y, x) for x, y in geom.exterior.coords],\n",
    "            color=\"blue\",\n",
    "            fill=True,\n",
    "            fill_opacity=0.4,\n",
    "            popup=\"Polygon Geometry\"\n",
    "        ).add_to(conus_map)\n",
    "\n",
    "display(conus_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075f430-6831-4cbc-a0ce-a4c6c38ea028",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in gdf_candidates_multihazard.iterrows():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f54b6-c7ed-45a0-91c4-4e0450bb3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row['outage geometry']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26686fe0-e278-4f05-aeb0-13f22b5c2978",
   "metadata": {},
   "source": [
    "TODO\n",
    "- Compile the concrete information that we know about this event\n",
    "- Determine what continuous data we have (imagery over time, space weather data (GIC and SuperMAG)\n",
    "- Can these 'layers' (need the weather and space weather spatiotemporal maps) yield network data and how might I explore its information over time?\n",
    "- get additional data - Geo-electric field\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50d7c0-f7c4-4c06-977a-18e58b90f684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e11c9d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/spwxr_network_new/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'state'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     candidate_events_gdf \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mGeoDataFrame(candidate_events_df, geometry\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutage geometry\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Filter for events in California\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     california_events \u001b[38;5;241m=\u001b[39m candidate_events_gdf[\u001b[43mcandidate_events_gdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalifornia\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(california_events)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/spwxr_network_new/lib/python3.9/site-packages/geopandas/geodataframe.py:1750\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;124;03m    If the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;124;03m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;124;03m    return a GeoDataFrame.\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1750\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# Custom logic to avoid waiting for pandas GH51895\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# result is not geometry dtype for multi-indexes\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1754\u001b[0m         pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_scalar(key)\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_geometry_type(result)\n\u001b[1;32m   1759\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniforge3/envs/spwxr_network_new/lib/python3.9/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/envs/spwxr_network_new/lib/python3.9/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'state'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Path to the pickle file\n",
    "pkl_file_path = '/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/candidate_multihazards_data/wildfire_space-weather_2023_candidates.pkl'\n",
    "\n",
    "# Load the pickle file using pandas\n",
    "candidate_events_df = pd.read_pickle(pkl_file_path)\n",
    "\n",
    "# If the DataFrame contains geospatial data, convert it to a GeoDataFrame\n",
    "if 'outage geometry' in candidate_events_df.columns:\n",
    "    candidate_events_gdf = gpd.GeoDataFrame(candidate_events_df, geometry='outage geometry')\n",
    "    # Filter for events in California\n",
    "    california_events = candidate_events_gdf[candidate_events_gdf['state'] == 'California']\n",
    "    print(california_events)\n",
    "else:\n",
    "    print(\"The DataFrame does not contain a 'geometry' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1046110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wildfire datetime</th>\n",
       "      <th>wildfire geometry</th>\n",
       "      <th>outage_start_time</th>\n",
       "      <th>outage_stop_time</th>\n",
       "      <th>customers affected</th>\n",
       "      <th>outage geometry</th>\n",
       "      <th>area affected</th>\n",
       "      <th>max SME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-07 04:10:00</td>\n",
       "      <td>POLYGON ((-97.5908 27.8072, -96.6931 30.5511, ...</td>\n",
       "      <td>2023-01-07 23:44:00</td>\n",
       "      <td>2023-01-08 02:29:00</td>\n",
       "      <td>9823</td>\n",
       "      <td>POLYGON ((-94.4578 31.03333, -94.45746 31.0334...</td>\n",
       "      <td>[(Texas, Jasper County), (Texas, Tyler County)...</td>\n",
       "      <td>[234.766418]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-11 15:41:00</td>\n",
       "      <td>POLYGON ((-82.4514 32.6894, -82.4681 32.9785, ...</td>\n",
       "      <td>2023-01-12 14:00:00</td>\n",
       "      <td>2023-01-13 03:00:00</td>\n",
       "      <td>162000</td>\n",
       "      <td>POLYGON ((-85.00256 31.00068, -85.00275 31.000...</td>\n",
       "      <td>[(Alabama, None), (Georgia, None)]</td>\n",
       "      <td>[503.498718]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-11 15:42:00</td>\n",
       "      <td>POLYGON ((-82.2786 29.3265, -82.2924 29.328, -...</td>\n",
       "      <td>2023-01-12 14:00:00</td>\n",
       "      <td>2023-01-13 03:00:00</td>\n",
       "      <td>162000</td>\n",
       "      <td>POLYGON ((-85.00256 31.00068, -85.00275 31.000...</td>\n",
       "      <td>[(Alabama, None), (Georgia, None)]</td>\n",
       "      <td>[503.498718]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-11 18:53:00</td>\n",
       "      <td>POLYGON ((-82.2929 29.3187, -91.0701 30.0902, ...</td>\n",
       "      <td>2023-01-12 14:00:00</td>\n",
       "      <td>2023-01-13 03:00:00</td>\n",
       "      <td>162000</td>\n",
       "      <td>POLYGON ((-85.00256 31.00068, -85.00275 31.000...</td>\n",
       "      <td>[(Alabama, None), (Georgia, None)]</td>\n",
       "      <td>[503.498718]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-11 18:54:00</td>\n",
       "      <td>POLYGON ((-82.7434 32.9896, -95.0548 33.8018, ...</td>\n",
       "      <td>2023-01-12 14:00:00</td>\n",
       "      <td>2023-01-13 03:00:00</td>\n",
       "      <td>162000</td>\n",
       "      <td>POLYGON ((-85.00256 31.00068, -85.00275 31.000...</td>\n",
       "      <td>[(Alabama, None), (Georgia, None)]</td>\n",
       "      <td>[503.498718]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-01-23 17:23:00</td>\n",
       "      <td>POLYGON ((-96.8342 28.6828, -104.7759 30.8824,...</td>\n",
       "      <td>2023-01-24 15:25:00</td>\n",
       "      <td>2023-01-26 20:20:00</td>\n",
       "      <td>100731</td>\n",
       "      <td>POLYGON ((-94.97884 29.67673, -94.97918 29.676...</td>\n",
       "      <td>[(Texas, Harris County), (, None)]</td>\n",
       "      <td>[767.988037]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-01-23 19:12:00</td>\n",
       "      <td>POLYGON ((-90.837 29.6284, -92.4218 29.6549, -...</td>\n",
       "      <td>2023-01-24 15:25:00</td>\n",
       "      <td>2023-01-26 20:20:00</td>\n",
       "      <td>100731</td>\n",
       "      <td>POLYGON ((-94.97884 29.67673, -94.97918 29.676...</td>\n",
       "      <td>[(Texas, Harris County), (, None)]</td>\n",
       "      <td>[767.988037]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-01-24 19:55:00</td>\n",
       "      <td>POINT (-99.9677 30.6221)</td>\n",
       "      <td>2023-01-25 03:30:00</td>\n",
       "      <td>2023-01-25 13:00:00</td>\n",
       "      <td>60958</td>\n",
       "      <td>POLYGON ((-88.85491 30.13553, -88.83936 30.101...</td>\n",
       "      <td>[(Arkansas, None), (Texas, None), (Louisiana, ...</td>\n",
       "      <td>[444.479645]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-02-20 19:21:00</td>\n",
       "      <td>POLYGON ((-97.9498 27.3197, -98.3125 27.551, -...</td>\n",
       "      <td>2023-02-21 13:14:00</td>\n",
       "      <td>2023-02-21 14:36:00</td>\n",
       "      <td>2547</td>\n",
       "      <td>POLYGON ((-81.15781 28.61222, -81.14608 28.612...</td>\n",
       "      <td>[(Florida, Orange County), (, None)]</td>\n",
       "      <td>[521.609131]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-03-22 20:54:00</td>\n",
       "      <td>POLYGON ((-114.9734 35.869, -114.9641 35.8799,...</td>\n",
       "      <td>2023-03-23 00:39:00</td>\n",
       "      <td>2023-03-23 00:40:00</td>\n",
       "      <td>11479</td>\n",
       "      <td>POLYGON ((-115.89692 36.84208, -115.89159 36.8...</td>\n",
       "      <td>[(Nevada, Clark County), (, None)]</td>\n",
       "      <td>[1162.580811]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-03-30 19:58:00</td>\n",
       "      <td>POLYGON ((-92.7692 34.4832, -92.7761 34.4912, ...</td>\n",
       "      <td>2023-03-31 18:00:00</td>\n",
       "      <td>2023-03-31 20:45:00</td>\n",
       "      <td>58368</td>\n",
       "      <td>POLYGON ((-90.95577 34.11871, -90.95451 34.117...</td>\n",
       "      <td>[(Arkansas, None)]</td>\n",
       "      <td>[1141.916382]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-03-31 19:03:00</td>\n",
       "      <td>POLYGON ((-97.5189 35.5637, -97.5215 35.5773, ...</td>\n",
       "      <td>2023-03-31 20:49:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>65000</td>\n",
       "      <td>POLYGON ((-87.89243 38.28285, -87.89334 38.282...</td>\n",
       "      <td>[(Illinois, None)]</td>\n",
       "      <td>[1141.916382]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-04-15 17:48:00</td>\n",
       "      <td>POLYGON ((-100.7977 32.8079, -108.5254 37.3928...</td>\n",
       "      <td>2023-04-16 07:17:00</td>\n",
       "      <td>2023-04-16 14:00:00</td>\n",
       "      <td>3900</td>\n",
       "      <td>POLYGON ((-88.85491 30.13553, -88.83936 30.101...</td>\n",
       "      <td>[(Texas, None), (Louisiana, None), (Arkansas, ...</td>\n",
       "      <td>[811.331543]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-04-15 19:44:00</td>\n",
       "      <td>POLYGON ((-98.0957 27.2058, -96.7593 29.2218, ...</td>\n",
       "      <td>2023-04-16 07:17:00</td>\n",
       "      <td>2023-04-16 14:00:00</td>\n",
       "      <td>3900</td>\n",
       "      <td>POLYGON ((-88.85491 30.13553, -88.83936 30.101...</td>\n",
       "      <td>[(Texas, None), (Louisiana, None), (Arkansas, ...</td>\n",
       "      <td>[811.331543]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-04-15 19:45:00</td>\n",
       "      <td>POLYGON ((-94.8123 32.2359, -96.5941 32.2881, ...</td>\n",
       "      <td>2023-04-16 07:17:00</td>\n",
       "      <td>2023-04-16 14:00:00</td>\n",
       "      <td>3900</td>\n",
       "      <td>POLYGON ((-88.85491 30.13553, -88.83936 30.101...</td>\n",
       "      <td>[(Texas, None), (Louisiana, None), (Arkansas, ...</td>\n",
       "      <td>[811.331543]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-04-15 19:46:00</td>\n",
       "      <td>POLYGON ((-95.3235 35.0513, -92.9702 36.9964, ...</td>\n",
       "      <td>2023-04-16 07:17:00</td>\n",
       "      <td>2023-04-16 14:00:00</td>\n",
       "      <td>3900</td>\n",
       "      <td>POLYGON ((-88.85491 30.13553, -88.83936 30.101...</td>\n",
       "      <td>[(Texas, None), (Louisiana, None), (Arkansas, ...</td>\n",
       "      <td>[811.331543]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-04-28 16:53:00</td>\n",
       "      <td>POLYGON ((-98.0256 26.127, -98.6578 28.4625, -...</td>\n",
       "      <td>2023-04-29 00:00:00</td>\n",
       "      <td>2023-04-29 20:00:00</td>\n",
       "      <td>168419</td>\n",
       "      <td>POLYGON ((-97.34144 25.92874, -97.34189 25.928...</td>\n",
       "      <td>[(Texas, Hidalgo County), (Texas, Cameron Coun...</td>\n",
       "      <td>[1058.05835]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-06-21 19:14:00</td>\n",
       "      <td>POLYGON ((-93.3361 31.0453, -93.3434 31.0461, ...</td>\n",
       "      <td>2023-06-22 02:30:00</td>\n",
       "      <td>2023-06-22 02:38:00</td>\n",
       "      <td>6242</td>\n",
       "      <td>POLYGON ((-92.72474 33.01439, -92.72365 33.014...</td>\n",
       "      <td>[(Louisiana, None)]</td>\n",
       "      <td>[566.164673]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-06-23 19:04:00</td>\n",
       "      <td>POINT (-88.2926 46.5819)</td>\n",
       "      <td>2023-06-24 18:17:00</td>\n",
       "      <td>2023-06-24 23:25:00</td>\n",
       "      <td>5200</td>\n",
       "      <td>POLYGON ((-87.1127 47.818, -87.0631 47.79999, ...</td>\n",
       "      <td>[(Michigan, None)]</td>\n",
       "      <td>[876.719055]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-06-24 16:22:00</td>\n",
       "      <td>POLYGON ((-84.4855 42.7173, -85.6695 42.8244, ...</td>\n",
       "      <td>2023-06-24 18:17:00</td>\n",
       "      <td>2023-06-24 23:25:00</td>\n",
       "      <td>5200</td>\n",
       "      <td>POLYGON ((-87.1127 47.818, -87.0631 47.79999, ...</td>\n",
       "      <td>[(Michigan, None)]</td>\n",
       "      <td>[918.188354]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2023-06-24 16:23:00</td>\n",
       "      <td>POLYGON ((-87.4079 39.3433, -87.1446 41.6283, ...</td>\n",
       "      <td>2023-06-24 18:17:00</td>\n",
       "      <td>2023-06-24 23:25:00</td>\n",
       "      <td>5200</td>\n",
       "      <td>POLYGON ((-87.1127 47.818, -87.0631 47.79999, ...</td>\n",
       "      <td>[(Michigan, None)]</td>\n",
       "      <td>[918.188354]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2023-06-24 16:24:00</td>\n",
       "      <td>POLYGON ((-85.4864 34.8794, -90.4861 35.6989, ...</td>\n",
       "      <td>2023-06-25 06:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>120000</td>\n",
       "      <td>POLYGON ((-83.98762 36.58947, -83.98724 36.589...</td>\n",
       "      <td>[(Tennessee, None)]</td>\n",
       "      <td>[918.188354]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2023-06-24 16:25:00</td>\n",
       "      <td>POLYGON ((-88.2356 31.5966, -91.9492 32.56, -9...</td>\n",
       "      <td>2023-06-25 06:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>120000</td>\n",
       "      <td>POLYGON ((-83.98762 36.58947, -83.98724 36.589...</td>\n",
       "      <td>[(Tennessee, None)]</td>\n",
       "      <td>[918.188354]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2023-06-25 07:45:00</td>\n",
       "      <td>POLYGON ((-83.43 31.1678, -83.4403 31.1694, -9...</td>\n",
       "      <td>2023-06-25 06:45:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>120000</td>\n",
       "      <td>POLYGON ((-83.98762 36.58947, -83.98724 36.589...</td>\n",
       "      <td>[(Tennessee, None)]</td>\n",
       "      <td>[1481.621216]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2023-06-28 15:50:00</td>\n",
       "      <td>POLYGON ((-76.463 40.1176, -85.1245 40.5572, -...</td>\n",
       "      <td>2023-06-29 15:42:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>140000</td>\n",
       "      <td>POLYGON ((-86.3296 38.1818, -86.33037 38.1821,...</td>\n",
       "      <td>[(Indiana, None)]</td>\n",
       "      <td>[632.900085]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2023-07-19 19:10:00</td>\n",
       "      <td>POLYGON ((-94.4801 29.6233, -92.3921 31.4402, ...</td>\n",
       "      <td>2023-07-20 16:30:00</td>\n",
       "      <td>2023-07-22 12:17:00</td>\n",
       "      <td>35257</td>\n",
       "      <td>POLYGON ((-81.09538 31.52098, -81.0971 31.5197...</td>\n",
       "      <td>[(Georgia, None)]</td>\n",
       "      <td>[305.050293]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2023-07-20 16:07:00</td>\n",
       "      <td>POLYGON ((-82.3547 29.4952, -88.0633 30.7362, ...</td>\n",
       "      <td>2023-07-20 16:30:00</td>\n",
       "      <td>2023-07-22 12:17:00</td>\n",
       "      <td>35257</td>\n",
       "      <td>POLYGON ((-81.09538 31.52098, -81.0971 31.5197...</td>\n",
       "      <td>[(Georgia, None)]</td>\n",
       "      <td>[275.894287]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2023-07-25 16:12:00</td>\n",
       "      <td>POLYGON ((-83.7897 39.4317, -86.0074 41.5292, ...</td>\n",
       "      <td>2023-07-26 14:51:00</td>\n",
       "      <td>2023-07-26 17:23:00</td>\n",
       "      <td>246000</td>\n",
       "      <td>POLYGON ((-82.91134 42.35501, -82.91391 42.354...</td>\n",
       "      <td>[(Michigan, Oakland County), (Michigan, Wayne ...</td>\n",
       "      <td>[856.831055]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2023-07-27 18:14:00</td>\n",
       "      <td>POLYGON ((-79.17 36.6974, -81.4188 37.8684, -8...</td>\n",
       "      <td>2023-07-28 18:00:00</td>\n",
       "      <td>2023-07-30 07:45:00</td>\n",
       "      <td>52098</td>\n",
       "      <td>POLYGON ((-76.49388 36.55065, -76.50309 36.550...</td>\n",
       "      <td>[(West Virginia, None), (Virginia, None)]</td>\n",
       "      <td>[508.741852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2023-07-28 16:36:00</td>\n",
       "      <td>POLYGON ((-88.4287 41.3937, -90.8304 41.4754, ...</td>\n",
       "      <td>2023-07-28 22:58:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>85000</td>\n",
       "      <td>POLYGON ((-87.12505 42.49368, -87.16596 42.493...</td>\n",
       "      <td>[(Wisconsin, Jefferson County), (Wisconsin, Wa...</td>\n",
       "      <td>[443.383423]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2023-07-28 18:56:00</td>\n",
       "      <td>POLYGON ((-94.3861 34.3002, -85.513 38.2711, -...</td>\n",
       "      <td>2023-07-28 18:00:00</td>\n",
       "      <td>2023-07-30 07:45:00</td>\n",
       "      <td>52098</td>\n",
       "      <td>POLYGON ((-76.49388 36.55065, -76.50309 36.550...</td>\n",
       "      <td>[(West Virginia, None), (Virginia, None)]</td>\n",
       "      <td>[443.383423]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2023-07-28 18:57:00</td>\n",
       "      <td>LINESTRING (-85.9198 41.7187, -85.825 41.7059)</td>\n",
       "      <td>2023-07-28 18:00:00</td>\n",
       "      <td>2023-07-30 07:45:00</td>\n",
       "      <td>52098</td>\n",
       "      <td>POLYGON ((-76.49388 36.55065, -76.50309 36.550...</td>\n",
       "      <td>[(West Virginia, None), (Virginia, None)]</td>\n",
       "      <td>[443.383423]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2023-09-24 20:04:00</td>\n",
       "      <td>POLYGON ((-100.446 30.8778, -100.6853 32.0443,...</td>\n",
       "      <td>2023-09-24 23:44:00</td>\n",
       "      <td>2023-09-26 20:45:00</td>\n",
       "      <td>130000</td>\n",
       "      <td>MULTIPOLYGON (((-97.99792 30.72035, -97.99709 ...</td>\n",
       "      <td>[(Texas, Dallas County), (Texas, Tarrant Count...</td>\n",
       "      <td>[637.28833]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2023-09-24 20:05:00</td>\n",
       "      <td>LINESTRING (-97.6467 36.9127, -98.1115 37.3566)</td>\n",
       "      <td>2023-09-24 23:44:00</td>\n",
       "      <td>2023-09-26 20:45:00</td>\n",
       "      <td>130000</td>\n",
       "      <td>MULTIPOLYGON (((-97.99792 30.72035, -97.99709 ...</td>\n",
       "      <td>[(Texas, Dallas County), (Texas, Tarrant Count...</td>\n",
       "      <td>[637.28833]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2023-09-25 03:50:00</td>\n",
       "      <td>POLYGON ((-93.9262 29.7266, -101.7795 31.3618,...</td>\n",
       "      <td>2023-09-25 05:12:00</td>\n",
       "      <td>2023-09-25 06:41:00</td>\n",
       "      <td>23380</td>\n",
       "      <td>POLYGON ((-91.01406 30.33003, -91.01483 30.329...</td>\n",
       "      <td>[(Louisiana, East Baton Rouge Parish), (, None)]</td>\n",
       "      <td>[1573.613037]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2023-09-25 03:51:00</td>\n",
       "      <td>LINESTRING (-90.614 34.549, -90.6094 34.5555)</td>\n",
       "      <td>2023-09-25 05:12:00</td>\n",
       "      <td>2023-09-25 06:41:00</td>\n",
       "      <td>23380</td>\n",
       "      <td>POLYGON ((-91.01406 30.33003, -91.01483 30.329...</td>\n",
       "      <td>[(Louisiana, East Baton Rouge Parish), (, None)]</td>\n",
       "      <td>[1573.613037]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2023-09-29 18:35:00</td>\n",
       "      <td>POLYGON ((-120.5171 37.0352, -122.2428 39.172,...</td>\n",
       "      <td>2023-09-29 22:16:00</td>\n",
       "      <td>2023-09-29 22:18:00</td>\n",
       "      <td>9082</td>\n",
       "      <td>POLYGON ((-122.06943 39.84053, -122.06874 39.8...</td>\n",
       "      <td>[(California, Butte County), (, None)]</td>\n",
       "      <td>[902.839355]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2023-10-04 17:05:00</td>\n",
       "      <td>POLYGON ((-95.2874 32.9278, -105.7873 34.2461,...</td>\n",
       "      <td>2023-10-04 22:01:00</td>\n",
       "      <td>2023-10-06 04:15:00</td>\n",
       "      <td>153000</td>\n",
       "      <td>POLYGON ((-98.42353 34.08284, -98.42235 34.082...</td>\n",
       "      <td>[(Texas, None)]</td>\n",
       "      <td>[705.591797]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2023-10-04 17:06:00</td>\n",
       "      <td>LINESTRING (-98.8426 29.0639, -98.8548 29.0659)</td>\n",
       "      <td>2023-10-04 22:01:00</td>\n",
       "      <td>2023-10-06 04:15:00</td>\n",
       "      <td>153000</td>\n",
       "      <td>POLYGON ((-98.42353 34.08284, -98.42235 34.082...</td>\n",
       "      <td>[(Texas, None)]</td>\n",
       "      <td>[705.591797]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2023-10-04 17:07:00</td>\n",
       "      <td>POLYGON ((-99.4458 27.8067, -99.4576 27.8087, ...</td>\n",
       "      <td>2023-10-04 22:01:00</td>\n",
       "      <td>2023-10-06 04:15:00</td>\n",
       "      <td>153000</td>\n",
       "      <td>POLYGON ((-98.42353 34.08284, -98.42235 34.082...</td>\n",
       "      <td>[(Texas, None)]</td>\n",
       "      <td>[705.591797]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2023-10-04 20:28:00</td>\n",
       "      <td>POLYGON ((-99.244 28.9953, -97.1092 30.5492, -...</td>\n",
       "      <td>2023-10-04 22:01:00</td>\n",
       "      <td>2023-10-06 04:15:00</td>\n",
       "      <td>153000</td>\n",
       "      <td>POLYGON ((-98.42353 34.08284, -98.42235 34.082...</td>\n",
       "      <td>[(Texas, None)]</td>\n",
       "      <td>[521.514771]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     wildfire datetime                                  wildfire geometry  \\\n",
       "0  2023-01-07 04:10:00  POLYGON ((-97.5908 27.8072, -96.6931 30.5511, ...   \n",
       "1  2023-01-11 15:41:00  POLYGON ((-82.4514 32.6894, -82.4681 32.9785, ...   \n",
       "2  2023-01-11 15:42:00  POLYGON ((-82.2786 29.3265, -82.2924 29.328, -...   \n",
       "3  2023-01-11 18:53:00  POLYGON ((-82.2929 29.3187, -91.0701 30.0902, ...   \n",
       "4  2023-01-11 18:54:00  POLYGON ((-82.7434 32.9896, -95.0548 33.8018, ...   \n",
       "5  2023-01-23 17:23:00  POLYGON ((-96.8342 28.6828, -104.7759 30.8824,...   \n",
       "6  2023-01-23 19:12:00  POLYGON ((-90.837 29.6284, -92.4218 29.6549, -...   \n",
       "7  2023-01-24 19:55:00                           POINT (-99.9677 30.6221)   \n",
       "8  2023-02-20 19:21:00  POLYGON ((-97.9498 27.3197, -98.3125 27.551, -...   \n",
       "9  2023-03-22 20:54:00  POLYGON ((-114.9734 35.869, -114.9641 35.8799,...   \n",
       "10 2023-03-30 19:58:00  POLYGON ((-92.7692 34.4832, -92.7761 34.4912, ...   \n",
       "11 2023-03-31 19:03:00  POLYGON ((-97.5189 35.5637, -97.5215 35.5773, ...   \n",
       "12 2023-04-15 17:48:00  POLYGON ((-100.7977 32.8079, -108.5254 37.3928...   \n",
       "13 2023-04-15 19:44:00  POLYGON ((-98.0957 27.2058, -96.7593 29.2218, ...   \n",
       "14 2023-04-15 19:45:00  POLYGON ((-94.8123 32.2359, -96.5941 32.2881, ...   \n",
       "15 2023-04-15 19:46:00  POLYGON ((-95.3235 35.0513, -92.9702 36.9964, ...   \n",
       "16 2023-04-28 16:53:00  POLYGON ((-98.0256 26.127, -98.6578 28.4625, -...   \n",
       "17 2023-06-21 19:14:00  POLYGON ((-93.3361 31.0453, -93.3434 31.0461, ...   \n",
       "18 2023-06-23 19:04:00                           POINT (-88.2926 46.5819)   \n",
       "19 2023-06-24 16:22:00  POLYGON ((-84.4855 42.7173, -85.6695 42.8244, ...   \n",
       "20 2023-06-24 16:23:00  POLYGON ((-87.4079 39.3433, -87.1446 41.6283, ...   \n",
       "21 2023-06-24 16:24:00  POLYGON ((-85.4864 34.8794, -90.4861 35.6989, ...   \n",
       "22 2023-06-24 16:25:00  POLYGON ((-88.2356 31.5966, -91.9492 32.56, -9...   \n",
       "23 2023-06-25 07:45:00  POLYGON ((-83.43 31.1678, -83.4403 31.1694, -9...   \n",
       "24 2023-06-28 15:50:00  POLYGON ((-76.463 40.1176, -85.1245 40.5572, -...   \n",
       "25 2023-07-19 19:10:00  POLYGON ((-94.4801 29.6233, -92.3921 31.4402, ...   \n",
       "26 2023-07-20 16:07:00  POLYGON ((-82.3547 29.4952, -88.0633 30.7362, ...   \n",
       "27 2023-07-25 16:12:00  POLYGON ((-83.7897 39.4317, -86.0074 41.5292, ...   \n",
       "28 2023-07-27 18:14:00  POLYGON ((-79.17 36.6974, -81.4188 37.8684, -8...   \n",
       "29 2023-07-28 16:36:00  POLYGON ((-88.4287 41.3937, -90.8304 41.4754, ...   \n",
       "30 2023-07-28 18:56:00  POLYGON ((-94.3861 34.3002, -85.513 38.2711, -...   \n",
       "31 2023-07-28 18:57:00     LINESTRING (-85.9198 41.7187, -85.825 41.7059)   \n",
       "32 2023-09-24 20:04:00  POLYGON ((-100.446 30.8778, -100.6853 32.0443,...   \n",
       "33 2023-09-24 20:05:00    LINESTRING (-97.6467 36.9127, -98.1115 37.3566)   \n",
       "34 2023-09-25 03:50:00  POLYGON ((-93.9262 29.7266, -101.7795 31.3618,...   \n",
       "35 2023-09-25 03:51:00      LINESTRING (-90.614 34.549, -90.6094 34.5555)   \n",
       "36 2023-09-29 18:35:00  POLYGON ((-120.5171 37.0352, -122.2428 39.172,...   \n",
       "37 2023-10-04 17:05:00  POLYGON ((-95.2874 32.9278, -105.7873 34.2461,...   \n",
       "38 2023-10-04 17:06:00    LINESTRING (-98.8426 29.0639, -98.8548 29.0659)   \n",
       "39 2023-10-04 17:07:00  POLYGON ((-99.4458 27.8067, -99.4576 27.8087, ...   \n",
       "40 2023-10-04 20:28:00  POLYGON ((-99.244 28.9953, -97.1092 30.5492, -...   \n",
       "\n",
       "     outage_start_time    outage_stop_time  customers affected  \\\n",
       "0  2023-01-07 23:44:00 2023-01-08 02:29:00                9823   \n",
       "1  2023-01-12 14:00:00 2023-01-13 03:00:00              162000   \n",
       "2  2023-01-12 14:00:00 2023-01-13 03:00:00              162000   \n",
       "3  2023-01-12 14:00:00 2023-01-13 03:00:00              162000   \n",
       "4  2023-01-12 14:00:00 2023-01-13 03:00:00              162000   \n",
       "5  2023-01-24 15:25:00 2023-01-26 20:20:00              100731   \n",
       "6  2023-01-24 15:25:00 2023-01-26 20:20:00              100731   \n",
       "7  2023-01-25 03:30:00 2023-01-25 13:00:00               60958   \n",
       "8  2023-02-21 13:14:00 2023-02-21 14:36:00                2547   \n",
       "9  2023-03-23 00:39:00 2023-03-23 00:40:00               11479   \n",
       "10 2023-03-31 18:00:00 2023-03-31 20:45:00               58368   \n",
       "11 2023-03-31 20:49:00                 NaT               65000   \n",
       "12 2023-04-16 07:17:00 2023-04-16 14:00:00                3900   \n",
       "13 2023-04-16 07:17:00 2023-04-16 14:00:00                3900   \n",
       "14 2023-04-16 07:17:00 2023-04-16 14:00:00                3900   \n",
       "15 2023-04-16 07:17:00 2023-04-16 14:00:00                3900   \n",
       "16 2023-04-29 00:00:00 2023-04-29 20:00:00              168419   \n",
       "17 2023-06-22 02:30:00 2023-06-22 02:38:00                6242   \n",
       "18 2023-06-24 18:17:00 2023-06-24 23:25:00                5200   \n",
       "19 2023-06-24 18:17:00 2023-06-24 23:25:00                5200   \n",
       "20 2023-06-24 18:17:00 2023-06-24 23:25:00                5200   \n",
       "21 2023-06-25 06:45:00                 NaT              120000   \n",
       "22 2023-06-25 06:45:00                 NaT              120000   \n",
       "23 2023-06-25 06:45:00                 NaT              120000   \n",
       "24 2023-06-29 15:42:00                 NaT              140000   \n",
       "25 2023-07-20 16:30:00 2023-07-22 12:17:00               35257   \n",
       "26 2023-07-20 16:30:00 2023-07-22 12:17:00               35257   \n",
       "27 2023-07-26 14:51:00 2023-07-26 17:23:00              246000   \n",
       "28 2023-07-28 18:00:00 2023-07-30 07:45:00               52098   \n",
       "29 2023-07-28 22:58:00                 NaT               85000   \n",
       "30 2023-07-28 18:00:00 2023-07-30 07:45:00               52098   \n",
       "31 2023-07-28 18:00:00 2023-07-30 07:45:00               52098   \n",
       "32 2023-09-24 23:44:00 2023-09-26 20:45:00              130000   \n",
       "33 2023-09-24 23:44:00 2023-09-26 20:45:00              130000   \n",
       "34 2023-09-25 05:12:00 2023-09-25 06:41:00               23380   \n",
       "35 2023-09-25 05:12:00 2023-09-25 06:41:00               23380   \n",
       "36 2023-09-29 22:16:00 2023-09-29 22:18:00                9082   \n",
       "37 2023-10-04 22:01:00 2023-10-06 04:15:00              153000   \n",
       "38 2023-10-04 22:01:00 2023-10-06 04:15:00              153000   \n",
       "39 2023-10-04 22:01:00 2023-10-06 04:15:00              153000   \n",
       "40 2023-10-04 22:01:00 2023-10-06 04:15:00              153000   \n",
       "\n",
       "                                      outage geometry  \\\n",
       "0   POLYGON ((-94.4578 31.03333, -94.45746 31.0334...   \n",
       "1   POLYGON ((-85.00256 31.00068, -85.00275 31.000...   \n",
       "2   POLYGON ((-85.00256 31.00068, -85.00275 31.000...   \n",
       "3   POLYGON ((-85.00256 31.00068, -85.00275 31.000...   \n",
       "4   POLYGON ((-85.00256 31.00068, -85.00275 31.000...   \n",
       "5   POLYGON ((-94.97884 29.67673, -94.97918 29.676...   \n",
       "6   POLYGON ((-94.97884 29.67673, -94.97918 29.676...   \n",
       "7   POLYGON ((-88.85491 30.13553, -88.83936 30.101...   \n",
       "8   POLYGON ((-81.15781 28.61222, -81.14608 28.612...   \n",
       "9   POLYGON ((-115.89692 36.84208, -115.89159 36.8...   \n",
       "10  POLYGON ((-90.95577 34.11871, -90.95451 34.117...   \n",
       "11  POLYGON ((-87.89243 38.28285, -87.89334 38.282...   \n",
       "12  POLYGON ((-88.85491 30.13553, -88.83936 30.101...   \n",
       "13  POLYGON ((-88.85491 30.13553, -88.83936 30.101...   \n",
       "14  POLYGON ((-88.85491 30.13553, -88.83936 30.101...   \n",
       "15  POLYGON ((-88.85491 30.13553, -88.83936 30.101...   \n",
       "16  POLYGON ((-97.34144 25.92874, -97.34189 25.928...   \n",
       "17  POLYGON ((-92.72474 33.01439, -92.72365 33.014...   \n",
       "18  POLYGON ((-87.1127 47.818, -87.0631 47.79999, ...   \n",
       "19  POLYGON ((-87.1127 47.818, -87.0631 47.79999, ...   \n",
       "20  POLYGON ((-87.1127 47.818, -87.0631 47.79999, ...   \n",
       "21  POLYGON ((-83.98762 36.58947, -83.98724 36.589...   \n",
       "22  POLYGON ((-83.98762 36.58947, -83.98724 36.589...   \n",
       "23  POLYGON ((-83.98762 36.58947, -83.98724 36.589...   \n",
       "24  POLYGON ((-86.3296 38.1818, -86.33037 38.1821,...   \n",
       "25  POLYGON ((-81.09538 31.52098, -81.0971 31.5197...   \n",
       "26  POLYGON ((-81.09538 31.52098, -81.0971 31.5197...   \n",
       "27  POLYGON ((-82.91134 42.35501, -82.91391 42.354...   \n",
       "28  POLYGON ((-76.49388 36.55065, -76.50309 36.550...   \n",
       "29  POLYGON ((-87.12505 42.49368, -87.16596 42.493...   \n",
       "30  POLYGON ((-76.49388 36.55065, -76.50309 36.550...   \n",
       "31  POLYGON ((-76.49388 36.55065, -76.50309 36.550...   \n",
       "32  MULTIPOLYGON (((-97.99792 30.72035, -97.99709 ...   \n",
       "33  MULTIPOLYGON (((-97.99792 30.72035, -97.99709 ...   \n",
       "34  POLYGON ((-91.01406 30.33003, -91.01483 30.329...   \n",
       "35  POLYGON ((-91.01406 30.33003, -91.01483 30.329...   \n",
       "36  POLYGON ((-122.06943 39.84053, -122.06874 39.8...   \n",
       "37  POLYGON ((-98.42353 34.08284, -98.42235 34.082...   \n",
       "38  POLYGON ((-98.42353 34.08284, -98.42235 34.082...   \n",
       "39  POLYGON ((-98.42353 34.08284, -98.42235 34.082...   \n",
       "40  POLYGON ((-98.42353 34.08284, -98.42235 34.082...   \n",
       "\n",
       "                                        area affected        max SME  \n",
       "0   [(Texas, Jasper County), (Texas, Tyler County)...   [234.766418]  \n",
       "1                  [(Alabama, None), (Georgia, None)]   [503.498718]  \n",
       "2                  [(Alabama, None), (Georgia, None)]   [503.498718]  \n",
       "3                  [(Alabama, None), (Georgia, None)]   [503.498718]  \n",
       "4                  [(Alabama, None), (Georgia, None)]   [503.498718]  \n",
       "5                  [(Texas, Harris County), (, None)]   [767.988037]  \n",
       "6                  [(Texas, Harris County), (, None)]   [767.988037]  \n",
       "7   [(Arkansas, None), (Texas, None), (Louisiana, ...   [444.479645]  \n",
       "8                [(Florida, Orange County), (, None)]   [521.609131]  \n",
       "9                  [(Nevada, Clark County), (, None)]  [1162.580811]  \n",
       "10                                 [(Arkansas, None)]  [1141.916382]  \n",
       "11                                 [(Illinois, None)]  [1141.916382]  \n",
       "12  [(Texas, None), (Louisiana, None), (Arkansas, ...   [811.331543]  \n",
       "13  [(Texas, None), (Louisiana, None), (Arkansas, ...   [811.331543]  \n",
       "14  [(Texas, None), (Louisiana, None), (Arkansas, ...   [811.331543]  \n",
       "15  [(Texas, None), (Louisiana, None), (Arkansas, ...   [811.331543]  \n",
       "16  [(Texas, Hidalgo County), (Texas, Cameron Coun...   [1058.05835]  \n",
       "17                                [(Louisiana, None)]   [566.164673]  \n",
       "18                                 [(Michigan, None)]   [876.719055]  \n",
       "19                                 [(Michigan, None)]   [918.188354]  \n",
       "20                                 [(Michigan, None)]   [918.188354]  \n",
       "21                                [(Tennessee, None)]   [918.188354]  \n",
       "22                                [(Tennessee, None)]   [918.188354]  \n",
       "23                                [(Tennessee, None)]  [1481.621216]  \n",
       "24                                  [(Indiana, None)]   [632.900085]  \n",
       "25                                  [(Georgia, None)]   [305.050293]  \n",
       "26                                  [(Georgia, None)]   [275.894287]  \n",
       "27  [(Michigan, Oakland County), (Michigan, Wayne ...   [856.831055]  \n",
       "28          [(West Virginia, None), (Virginia, None)]   [508.741852]  \n",
       "29  [(Wisconsin, Jefferson County), (Wisconsin, Wa...   [443.383423]  \n",
       "30          [(West Virginia, None), (Virginia, None)]   [443.383423]  \n",
       "31          [(West Virginia, None), (Virginia, None)]   [443.383423]  \n",
       "32  [(Texas, Dallas County), (Texas, Tarrant Count...    [637.28833]  \n",
       "33  [(Texas, Dallas County), (Texas, Tarrant Count...    [637.28833]  \n",
       "34   [(Louisiana, East Baton Rouge Parish), (, None)]  [1573.613037]  \n",
       "35   [(Louisiana, East Baton Rouge Parish), (, None)]  [1573.613037]  \n",
       "36             [(California, Butte County), (, None)]   [902.839355]  \n",
       "37                                    [(Texas, None)]   [705.591797]  \n",
       "38                                    [(Texas, None)]   [705.591797]  \n",
       "39                                    [(Texas, None)]   [705.591797]  \n",
       "40                                    [(Texas, None)]   [521.514771]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69bc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba048ad0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The geopandas.dataset has been deprecated and was removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m candidate_events_gdf \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mGeoDataFrame(candidate_events_df, geometry\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutage geometry\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Filter for events in California based on geometry\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m california_events \u001b[38;5;241m=\u001b[39m candidate_events_gdf[candidate_events_gdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutage geometry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mintersects(gpd\u001b[38;5;241m.\u001b[39mGeoSeries\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnaturalearth_lowres\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)[gpd\u001b[38;5;241m.\u001b[39mGeoSeries\u001b[38;5;241m.\u001b[39mfrom_file(gpd\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mget_path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnaturalearth_lowres\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalifornia\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munary_union)]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Display the filtered events\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(california_events)\n",
      "File \u001b[0;32m~/miniforge3/envs/spwxr_network_new/lib/python3.9/site-packages/geopandas/datasets/__init__.py:18\u001b[0m, in \u001b[0;36mget_path\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     12\u001b[0m error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe geopandas.dataset has been deprecated and was removed in GeoPandas \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0. You can get the original \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m data from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mne_message\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnatural\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mdataset\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39mnybb_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m _prev_available:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(error_msg)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe geopandas.dataset has been deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas removed in GeoPandas 1.0. New sample datasets are now available \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the geodatasets package (https://geodatasets.readthedocs.io/en/latest/)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: The geopandas.dataset has been deprecated and was removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/."
     ]
    }
   ],
   "source": [
    "# Read in a pickle file containing a GeoPandas DataFrame of candidate events\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Path to the pickle file\n",
    "pkl_file_path = '/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/candidate_multihazards_data/wildfire_space-weather_2023_candidates.pkl'\n",
    "\n",
    "# Load the pickle file using pandas\n",
    "candidate_events_df = pd.read_pickle(pkl_file_path)\n",
    "\n",
    "# If the DataFrame contains geospatial data, convert it to a GeoDataFrame\n",
    "if 'outage geometry' in candidate_events_df.columns:\n",
    "    candidate_events_gdf = gpd.GeoDataFrame(candidate_events_df, geometry='outage geometry')\n",
    "    \n",
    "    # Filter for events in California based on geometry\n",
    "    california_events = candidate_events_gdf[candidate_events_gdf['outage geometry'].intersects(gpd.GeoSeries.from_file(gpd.datasets.get_path('naturalearth_lowres'))[gpd.GeoSeries.from_file(gpd.datasets.get_path('naturalearth_lowres'))['name'] == 'California'].unary_union)]\n",
    "    \n",
    "    # Display the filtered events\n",
    "    print(california_events)\n",
    "    \n",
    "    # Optionally, save the filtered events to a new file\n",
    "    california_events.to_file('california_events.geojson', driver='GeoJSON')\n",
    "else:\n",
    "    print(\"The DataFrame does not contain a 'geometry' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f94dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bb8277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty GeoDataFrame\n",
      "Columns: [wildfire datetime, wildfire geometry, outage_start_time, outage_stop_time, customers affected, outage geometry, area affected, max SME]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/nsj8l6gd247cf7v6b5_pnsm00000gp/T/ipykernel_26489/1125324949.py:19: UserWarning: The indices of the left and right GeoSeries' are not equal, and therefore they will be aligned (reordering and/or introducing missing values) before executing the operation. If this alignment is the desired behaviour, you can silence this warning by passing 'align=True'. If you don't want alignment and protect yourself of accidentally aligning, you can pass 'align=False'.\n",
      "  california_events = candidate_events_gdf[candidate_events_gdf['outage geometry'].intersects(california_boundary)]\n"
     ]
    }
   ],
   "source": [
    "# Filter candidate events based on intersection with California using gdf_states\n",
    "import geopandas as gpd\n",
    "\n",
    "# Path to the shapefile for states\n",
    "gdf_states = gpd.read_file('/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/location_data/Census_Bureau_Data/tl_2023_us_state/tl_2023_us_state.shp')\n",
    "\n",
    "# Ensure the GeoDataFrame is in the same CRS as the candidate events\n",
    "if candidate_events_gdf.crs is None:\n",
    "    candidate_events_gdf.set_crs(\"EPSG:4326\", inplace=True)  # Set default CRS if missing\n",
    "\n",
    "if gdf_states.crs is None:\n",
    "    gdf_states.set_crs(\"EPSG:4326\", inplace=True)  # Set default CRS if missing\n",
    "\n",
    "if candidate_events_gdf.crs != gdf_states.crs:\n",
    "    candidate_events_gdf = candidate_events_gdf.to_crs(gdf_states.crs)\n",
    "\n",
    "# Filter for California\n",
    "california_boundary = gdf_states[gdf_states['NAME'] == 'California']\n",
    "california_events = candidate_events_gdf[candidate_events_gdf['outage geometry'].intersects(california_boundary)]\n",
    "\n",
    "# Display the filtered events\n",
    "print(california_events)\n",
    "\n",
    "# Optionally, save the filtered events to a new file\n",
    "california_events.to_file('../data/california_events.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf6f0ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/nsj8l6gd247cf7v6b5_pnsm00000gp/T/ipykernel_26489/2377627761.py:1: UserWarning: The indices of the left and right GeoSeries' are not equal, and therefore they will be aligned (reordering and/or introducing missing values) before executing the operation. If this alignment is the desired behaviour, you can silence this warning by passing 'align=True'. If you don't want alignment and protect yourself of accidentally aligning, you can pass 'align=False'.\n",
      "  california_events = candidate_events_gdf[candidate_events_gdf['outage geometry'].intersects(california_boundary['geometry'])]\n"
     ]
    }
   ],
   "source": [
    "california_events = candidate_events_gdf[candidate_events_gdf['outage geometry'].intersects(california_boundary['geometry'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c16fca6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wildfire datetime</th>\n",
       "      <th>wildfire geometry</th>\n",
       "      <th>outage_start_time</th>\n",
       "      <th>outage_stop_time</th>\n",
       "      <th>customers affected</th>\n",
       "      <th>outage geometry</th>\n",
       "      <th>area affected</th>\n",
       "      <th>max SME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty GeoDataFrame\n",
       "Columns: [wildfire datetime, wildfire geometry, outage_start_time, outage_stop_time, customers affected, outage geometry, area affected, max SME]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "california_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd408c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39be66f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   REGION DIVISION STATEFP   STATENS GEOID      GEOIDFQ STUSPS           NAME  \\\n",
      "0       3        5      54  01779805    54  0400000US54     WV  West Virginia   \n",
      "2       2        3      17  01779784    17  0400000US17     IL       Illinois   \n",
      "3       2        4      27  00662849    27  0400000US27     MN      Minnesota   \n",
      "15      2        3      55  01779806    55  0400000US55     WI      Wisconsin   \n",
      "24      2        3      39  01085497    39  0400000US39     OH           Ohio   \n",
      "25      3        7      48  01779801    48  0400000US48     TX          Texas   \n",
      "28      3        7      40  01102857    40  0400000US40     OK       Oklahoma   \n",
      "29      3        6      47  01325873    47  0400000US47     TN      Tennessee   \n",
      "43      3        7      05  00068085    05  0400000US05     AR       Arkansas   \n",
      "44      3        6      28  01779790    28  0400000US28     MS    Mississippi   \n",
      "45      2        4      29  01779791    29  0400000US29     MO       Missouri   \n",
      "47      2        4      20  00481813    20  0400000US20     KS         Kansas   \n",
      "48      2        3      18  00448508    18  0400000US18     IN        Indiana   \n",
      "54      2        4      19  01779785    19  0400000US19     IA           Iowa   \n",
      "\n",
      "   LSAD  MTFCC FUNCSTAT         ALAND       AWATER     INTPTLAT      INTPTLON  \\\n",
      "0    00  G4000        A   62266499712    489003081  +38.6472854  -080.6183274   \n",
      "2    00  G4000        A  143778366814   6216688589  +40.1028754  -089.1526108   \n",
      "3    00  G4000        A  206244555303  18937471947  +46.3159573  -094.1996043   \n",
      "15   00  G4000        A  140292627460  29343084365  +44.6309071  -089.7093916   \n",
      "24   00  G4000        A  105823831336  10274524796  +40.4149297  -082.7119975   \n",
      "25   00  G4000        A  676686238592  18982083586  +31.4347032  -099.2818238   \n",
      "28   00  G4000        A  177664484361   3373395450  +35.5900815  -097.4867789   \n",
      "29   00  G4000        A  106792311478   2322248149  +35.8584600  -086.3496339   \n",
      "43   00  G4000        A  134660466558   3122251184  +34.8955256  -092.4446262   \n",
      "44   00  G4000        A  121533540877   3914738613  +32.6864714  -089.6561377   \n",
      "45   00  G4000        A  178052260322   2487519141  +38.3507500  -092.4567826   \n",
      "47   00  G4000        A  211753777631   1345707708  +38.4985464  -098.3834298   \n",
      "48   00  G4000        A   92786613552   1543998356  +39.9013136  -086.2919129   \n",
      "54   00  G4000        A  144659283871   1086402333  +42.0700243  -093.4933473   \n",
      "\n",
      "                                             geometry  \n",
      "0   POLYGON ((-77.75438 39.33346, -77.75422 39.333...  \n",
      "2   POLYGON ((-87.89243 38.28285, -87.89334 38.282...  \n",
      "3   POLYGON ((-95.31991 48.99892, -95.31778 48.998...  \n",
      "15  POLYGON ((-89.55482 46.2279, -89.5548 46.2279,...  \n",
      "24  POLYGON ((-80.51915 40.90033, -80.51914 40.899...  \n",
      "25  POLYGON ((-98.42353 34.08284, -98.42235 34.082...  \n",
      "28  POLYGON ((-95.78676 36.99931, -95.78601 36.999...  \n",
      "29  POLYGON ((-83.98762 36.58947, -83.98724 36.589...  \n",
      "43  POLYGON ((-90.95577 34.11871, -90.95451 34.117...  \n",
      "44  POLYGON ((-88.43138 32.22767, -88.4318 32.2243...  \n",
      "45  POLYGON ((-93.37439 40.58033, -93.37429 40.580...  \n",
      "47  POLYGON ((-94.61764 37.76124, -94.61766 37.760...  \n",
      "48  POLYGON ((-86.3296 38.1818, -86.33037 38.1821,...  \n",
      "54  POLYGON ((-95.86095 43.50004, -95.85979 43.500...  \n"
     ]
    }
   ],
   "source": [
    "# Create a GeoDataFrame with selected states and their geometries\n",
    "import geopandas as gpd\n",
    "\n",
    "# List of states to include\n",
    "selected_states = [\n",
    "    \"Texas\", \"Oklahoma\", \"Arkansas\", \"Kansas\", \"Missouri\", \"Iowa\", \"Minnesota\", \n",
    "    \"Wisconsin\", \"Illinois\", \"Indiana\", \"Mississippi\", \"Tennessee\", \"Ohio\", \"West Virginia\"\n",
    "]\n",
    "\n",
    "# Path to the shapefile for states\n",
    "gdf_states = gpd.read_file('/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/location_data/Census_Bureau_Data/tl_2023_us_state/tl_2023_us_state.shp')\n",
    "\n",
    "# Filter the GeoDataFrame for the selected states\n",
    "selected_states_gdf = gdf_states[gdf_states['NAME'].isin(selected_states)]\n",
    "\n",
    "# Save the filtered GeoDataFrame to a new file\n",
    "output_path = '../data/selected_states_MarchApril2023_event.geojson'\n",
    "selected_states_gdf.to_file(output_path, driver='GeoJSON')\n",
    "\n",
    "# Display the filtered GeoDataFrame\n",
    "print(selected_states_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4b9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spwxr_network_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
