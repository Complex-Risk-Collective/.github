{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef95525",
   "metadata": {},
   "source": [
    "This script is an experiment in analyzing multi-hazard impacts on the power grid\n",
    "\n",
    "The goal of this script is to identify a candidate set of wildfire + space weather events\n",
    "\n",
    "Approach (implemented)\n",
    "- Get wildfire point \n",
    "- Choose radius of some sort\n",
    "- Get any power grid disturbance that occurred within that time and circle\n",
    "- Inspect space weather data for preceding day to current time\n",
    "- Store full multi-hazard data for the candidate event and study forensically\n",
    "\n",
    "Possibility of a different approach (not implemented):\n",
    "- Read in set of wildfire events from MODIS or Suomi\n",
    "- Get time period, get spatial extent (GIS) -> visualize it\n",
    "- Highest level: get space weather indices\n",
    "    - more granular is to get magnetometers and GNSS data (GIMs, likely) in the area of the fire \n",
    "- Get HIFLD grid affected based on the wildfire\n",
    "- (optional) get observed power grid disturbances that overlap in space and time\n",
    "- Record the candidate event for forensic analysis (social media, interviews, grey literature exploration)\n",
    "\n",
    "\n",
    "TODO\n",
    "- the connection may very well come from space weather effects on communications during wildfire events (need to incorporate radiation data to represent space weather) \n",
    "- explore emdat database\n",
    "- add sophistication to wildfire region processing\n",
    "- add terrestrial weather to search\n",
    "- explore 2024 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b17e7d",
   "metadata": {},
   "source": [
    "#### Dependencies and Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77097901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "from datetime import datetime, time\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "# import contextily as ctx\n",
    "\n",
    "\n",
    "from supermag_api.supermag_api import *\n",
    "\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9447d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sm_one_minute(start,end):\n",
    "\n",
    "    '''\n",
    "\n",
    "    \n",
    "    Dependencies\n",
    "        supermag-api downloadable from https://supermag.jhuapl.edu/mag/?fidelity=low&start=2001-01-01T00%3A00%3A00.000Z&interval=1%3A00%3A00&tab=api\n",
    "            --> one should move this to the working directory and rename to supermag_api\n",
    "        datetime\n",
    "            \n",
    "    Notes\n",
    "        it appears this will only work for a few-day request; for longer periods, need to download the data directly\n",
    "            \n",
    "    '''\n",
    "\n",
    "\n",
    "#     import supermag_api\n",
    "#     import datetime\n",
    "\n",
    "    start_datetime = pd.to_datetime(start)\n",
    "    end_datetime = pd.to_datetime(end)\n",
    "    \n",
    "    # get the number of days between start and end\n",
    "    num_days = (end_datetime - start_datetime).days\n",
    "    print('the number of days that will be requested is {}'.format(num_days))\n",
    "    input('Press Enter if you are sure you want to ping the SuperMAG API for this...')\n",
    "    \n",
    "    # get correct format for 'start date'\n",
    "    # date_obj_start = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n",
    "    # formatted_start = date_obj_start.strftime('%Y-%m-%dT%H:%M')\n",
    "    date_obj_start = datetime.datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n",
    "    formatted_start = date_obj_start.strftime('%Y-%m-%dT%H:%M')\n",
    "\n",
    "    (status,df_smidxs) = SuperMAGGetIndices('rymc1012',formatted_start,86400*num_days,'SME, SMU, SML, SMR')\n",
    "\n",
    "    datetimes_sm = [pd.to_datetime(t, unit='s') for t in df_smidxs['tval']]\n",
    "\n",
    "    df_smidxs['datetimes'] = datetimes_sm\n",
    "    df_smidxs = df_smidxs.set_index(['datetimes'])\n",
    "    df_smidxs.index = pd.to_datetime(df_smidxs.index)\n",
    "    \n",
    "    return df_smidxs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a509d0",
   "metadata": {},
   "source": [
    "#### Wildfire database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d970b9",
   "metadata": {},
   "source": [
    "Some things I'm learning about wildfire data\n",
    "- MODIS might be best available given coverage\n",
    "- There is a NASA digital twin project https://ideas-digitaltwin.jpl.nasa.gov/airquality/, but it is focused on air quality and thus the atmospheric emissions associated with wildfire, whereas for this work I need the wildfires themselves (not their emissions which then mix with the atmosphere and its dynamics)\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c59c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASA FIRMS data https://firms.modaps.eosdis.nasa.gov\n",
    "\n",
    "\n",
    "# National Interagency Fire Center https://www.nifc.gov/\n",
    "file_path_nifc_wildfire = \"/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/wildfire_data/WFIGS_Incident_Locations_6818922478522849878.geojson\"\n",
    "file_path_modis_wildfire = \"/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/wildfire_data/DL_FIRE_M-C61_530609/fire_archive_M-C61_530609.shp\"\n",
    "# file_path_suomi_wildfire = \"/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/wildfire_data/DL_FIRE_SV-C2_530613/fire_archive_SV-C2_530613.shp\"\n",
    "\n",
    "# gdf_nifc_wildfire = gpd.read_file(file_path_nifc_wildfire)\n",
    "gdf_modis_wildfire = gpd.read_file(file_path_modis_wildfire)\n",
    "# gdf_suomi_wildfire = gpd.read_file(file_path_suomi_wildfire)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81dbdc-0fc5-435d-add1-cac064bba8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of the wildfire data is: {:.2f} MB'.format(sys.getsizeof(gdf_modis_wildfire) / 10**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the first few rows\n",
    "print(gdf_modis_wildfire.head())\n",
    "\n",
    "# Check the geometry type (e.g., Point, Polygon, LineString)\n",
    "print(gdf_modis_wildfire.geom_type.unique())\n",
    "\n",
    "# Get summary of the attributes\n",
    "print(gdf_modis_wildfire.info())\n",
    "\n",
    "# # Plot the shapefile (quick visualization)\n",
    "# gdf.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ACQ_DATE' in gdf_modis_wildfire.columns and 'ACQ_TIME' in gdf_modis_wildfire.columns:\n",
    "    gdf_modis_wildfire['datetime'] = (\n",
    "        gdf_modis_wildfire['ACQ_DATE'].astype(str) + ' ' + gdf_modis_wildfire['ACQ_TIME'].astype(str)\n",
    "    )\n",
    "    gdf_modis_wildfire['datetime'] = pd.to_datetime(gdf_modis_wildfire['datetime'])\n",
    "else:\n",
    "    raise ValueError(\"Expected 'ACQ_DATE' and 'ACQ_TIME' columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f09dc17",
   "metadata": {},
   "source": [
    "### (optional) visualize the wildfire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e282fc-6488-47fc-ab6d-94f5239c60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Convert GeoDataFrame to GeoJSON-like dictionary with timestamp for Folium\n",
    "features = []\n",
    "for _, row in gdf_modis_wildfire.iterrows():\n",
    "    feature = {\n",
    "        'type': 'Feature',\n",
    "        'geometry': row['geometry'].__geo_interface__,\n",
    "        'properties': {\n",
    "            'time': row['datetime'].isoformat(),  # Use ISO format for timestamps\n",
    "            'popup': f\"Acquired: {row['datetime']}\"\n",
    "        }\n",
    "    }\n",
    "    features.append(feature)\n",
    "\n",
    "# Create the Timestamped GeoJSON layer\n",
    "timestamped_geojson = TimestampedGeoJson(\n",
    "    {'type': 'FeatureCollection', 'features': features},\n",
    "    period='PT1H',  # Period for animation: 1 hour\n",
    "    add_last_point=False,\n",
    "    auto_play=False,\n",
    "    loop=True,\n",
    "    max_speed=1,\n",
    "    loop_button=True,\n",
    "    date_options='YYYY-MM-DD HH:mm:ss',\n",
    "    time_slider_drag_update=True\n",
    ")\n",
    "\n",
    "# Initialize a Folium map centered on the dataset's centroid\n",
    "m = folium.Map(\n",
    "    location=[gdf_modis_wildfire.geometry.y.mean(), gdf_modis_wildfire.geometry.x.mean()],\n",
    "    zoom_start=6, tiles='CartoDB positron'\n",
    ")\n",
    "\n",
    "# Add the timestamped layer to the map\n",
    "timestamped_geojson.add_to(m)\n",
    "\n",
    "# Save the map as an HTML file and display it\n",
    "m.save('interactive_map.html')\n",
    "\n",
    "# If running in Jupyter, display the map directly\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aaacca",
   "metadata": {},
   "source": [
    "### Create a list of features grouped by unique timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for dt, group in gdf_modis_wildfire.groupby('datetime'):\n",
    "    feature = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': [\n",
    "            {\n",
    "                'type': 'Feature',\n",
    "                'geometry': row['geometry'].__geo_interface__,\n",
    "                'properties': {\n",
    "                    'time': dt.isoformat(),  # Set timestamp in ISO format\n",
    "                    'popup': f\"Acquired: {dt}\"\n",
    "                }\n",
    "            }\n",
    "            for _, row in group.iterrows()\n",
    "        ]\n",
    "    }\n",
    "    features.append(feature)\n",
    "\n",
    "\n",
    "# # Create the TimestampedGeoJson with these features\n",
    "# timestamped_geojson = TimestampedGeoJson(\n",
    "#     {'type': 'FeatureCollection', 'features': features},\n",
    "#     period='PT1H',              # Set period of time for each step (1 hour)\n",
    "#     add_last_point=False,        # Ensure only current points appear\n",
    "#     auto_play=False,              # Automatically start animation\n",
    "#     loop=True,                   # Loop the animation\n",
    "#     max_speed=1,                 # Normal animation speed\n",
    "#     loop_button=True,            # Allow user to loop animation\n",
    "#     date_options='YYYY-MM-DD HH:mm:ss',  # Display format for timestamps\n",
    "#     time_slider_drag_update=True # Update on dragging time slider\n",
    "# )\n",
    "\n",
    "# # Initialize the Folium map centered on the dataset's centroid\n",
    "# m = folium.Map(\n",
    "#     location=[gdf_modis_wildfire.geometry.y.mean(), gdf_modis_wildfire.geometry.x.mean()],\n",
    "#     zoom_start=6, \n",
    "#     tiles='CartoDB positron'  # Light basemap\n",
    "# )\n",
    "\n",
    "# # Add the timestamped GeoJSON to the map\n",
    "# timestamped_geojson.add_to(m)\n",
    "\n",
    "# # Save the map as an HTML file and display it\n",
    "# m.save('interactive_map.html')\n",
    "\n",
    "# # If running in Jupyter, display the map directly\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2063916a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "849a9b35",
   "metadata": {},
   "source": [
    "#### Optionally read in the HIFLD data for power grid transmission lines\n",
    "uses HIFLD data: https://hifld-geoplatform.hub.arcgis.com/datasets/geoplatform::transmission-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_powergrid = \"/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/physical_grid_data/U.S._Electric_Power_Transmission_Lines.geojson\"\n",
    "gdf_powergrid = gpd.read_file(file_path_powergrid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff248a-b2eb-46a0-8760-e60135d01090",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of the power grid data is: {:.2f} MB'.format(sys.getsizeof(gdf_powergrid) / 10**6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5e07d",
   "metadata": {},
   "source": [
    "#### Power Grid Disturbance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d68cc2",
   "metadata": {},
   "source": [
    "Candidates\n",
    "- [new dataset from EAGLE-I](https://www.nature.com/articles/s41597-024-03095-5) (Data are available in the Figshare repository at https://doi.org/10.6084/m9.figshare.24237376 or via archive request from the eagle-I website)\n",
    "- [DOE electric disturbance events OE-417](https://www.oe.netl.doe.gov/OE417_annual_summary.aspx)\n",
    "\n",
    "\n",
    "New Candidate: [NASA Black Marble](https://blackmarble.gsfc.nasa.gov/) (will require more thinking about how we might use these data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in 2023 data\n",
    "outage_directory = '/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/outage_data/'\n",
    "oe417 = pd.read_excel(os.path.join(outage_directory,'DOE-OE-417/2023_Annual_Summary.xlsx'),header=1)\n",
    "oe417 = oe417.dropna(subset=['Time Event Began'])\n",
    "oe417 = oe417.iloc[:-2]\n",
    "for o in range(len(oe417['Number of Customers Affected'])):\n",
    "    if oe417['Number of Customers Affected'].iloc[o]=='Unknown':\n",
    "        oe417['Number of Customers Affected'].iloc[o] = np.nan\n",
    "    elif type(oe417['Number of Customers Affected'].iloc[o]) == str: \n",
    "        oe417['Number of Customers Affected'].iloc[o] = int(oe417['Number of Customers Affected'].iloc[o])\n",
    "    \n",
    "oe417 = oe417[oe417['Number of Customers Affected']>1000]\n",
    "\n",
    "eaglei = pd.read_csv(os.path.join(outage_directory,'EAGLE-I/eaglei_outages_2023.csv'))\n",
    "# limit the data to >1000 customers out\n",
    "eaglei = eaglei[eaglei['customers_out']>1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b71b26-a903-4c18-b352-f91ee59dcce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of the oe417 data is: {:.2f} MB'.format(sys.getsizeof(oe417) / 10**6))\n",
    "print('size of the eaglei data is: {:.2f} MB'.format(sys.getsizeof(eaglei) / 10**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b24d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oe417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def get_datetime_oe417(date,time):\n",
    "    # try: \n",
    "    #     dt = datetime.combine(str_date,dt_time)\n",
    "    # except: \n",
    "    #     try:\n",
    "    #         date_obj = datetime.strptime(str_date, '%m/%d/%Y')\n",
    "    #         # dt = datetime.combine(date_obj, dt_time)\n",
    "    #         dt = parse(date_obj+' '+dt_time)\n",
    "    #     except:\n",
    "    #         dt = np.nan\n",
    "\n",
    "    if type(date) == str:\n",
    "        date_obj = datetime.strptime(date, '%m/%d/%Y')\n",
    "        dt = datetime.combine(date_obj,time)\n",
    "    else:\n",
    "        dt = datetime.combine(date,time)\n",
    "    \n",
    "    return dt\n",
    "\n",
    "# Regular expression to capture 'State: County' pairs\n",
    "pattern = r'(\\w+):\\s*([\\w\\s-]+)'\n",
    "\n",
    "# Function to extract states and counties from each row\n",
    "# def extract_state_county_pairs(row):\n",
    "#     pairs = []\n",
    "#     # Split by semicolon to handle multiple state-county pairs\n",
    "#     entries = row.split(';')\n",
    "#     for entry in entries:\n",
    "#         # Use regex to extract state and county\n",
    "#         match = re.match(r'(\\w+):\\s*(.*)', entry.strip())\n",
    "#         if match:\n",
    "#             state = match.group(1)\n",
    "#             county = match.group(2)\n",
    "#             pairs.append((state, county))\n",
    "#     return pairs\n",
    "\n",
    "def extract_state_county_pairs(location_str):\n",
    "    pairs = []\n",
    "\n",
    "    # Split by ';' to handle multiple state-county blocks\n",
    "    state_county_blocks = location_str.split(';')\n",
    "\n",
    "    for block in state_county_blocks:\n",
    "        block = block.strip()\n",
    "\n",
    "        # Case: Handle multiple consecutive states without counties (e.g., \"Alabama: Maine:\")\n",
    "        if re.fullmatch(r\"(.+?:)+\", block):\n",
    "            states = [state.strip() for state in block.split(':') if state.strip()]\n",
    "            for state in states:\n",
    "                pairs.append((state, None))  # Store each state with None for county\n",
    "            continue  # Skip further processing for this block\n",
    "\n",
    "        # Use regex to match \"State: County, County, ...\" or \"State:\"\n",
    "        match = re.match(r\"(.+?):\\s*(.*)\", block)\n",
    "\n",
    "        if match:\n",
    "            state = match.group(1).strip()\n",
    "            counties = match.group(2).strip()\n",
    "\n",
    "            if counties:  # If counties exist, split them by ','\n",
    "                counties_list = [county.strip() for county in counties.split(',')]\n",
    "                for county in counties_list:\n",
    "                    pairs.append((state, county))\n",
    "            else:  # If no counties are provided\n",
    "                pairs.append((state, None))\n",
    "        else:\n",
    "            # If the block contains only a state name\n",
    "            pairs.append((block, None))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def get_oe417_geometries(row_area_affected,gdf_states,gdf_counties):\n",
    "    '''\n",
    "        inputs: \n",
    "            - row of oe417 data for Area Affected (oe417['Area Affected'].iloc[x])\n",
    "            - data for states geometries\n",
    "            - data for county geometries\n",
    "        dependencies:\n",
    "            - extract_state_county_pairs function\n",
    "    '''\n",
    "    \n",
    "    row_states_counties = extract_state_county_pairs(row_area_affected)\n",
    "    all_geometries = []\n",
    "    for l in row_states_counties: \n",
    "        if not l[0]:\n",
    "            # skip the empty rows in the extracted data\n",
    "            continue\n",
    "        else:\n",
    "            if not l[1]:\n",
    "                # if there is no county data, then just use whole state\n",
    "                geom_data_tmp = gpd.GeoSeries(gdf_states['geometry'][gdf_states['NAME'] == l[0]].values)\n",
    "                if geom_data_tmp.empty:\n",
    "                    continue\n",
    "                if geom_data_tmp.apply(lambda geom: isinstance(geom, MultiPolygon)).all():\n",
    "#                     print('----> encountering multipolygon for {}, skipping'.format(l[0]))\n",
    "                    all_geometries.extend([polygon for polygon in geom_data_tmp.geometry[0].geoms])\n",
    "                else:\n",
    "                    all_geometries.extend( [Polygon(geom_data_tmp.geometry[0])] )\n",
    "            else:\n",
    "                # if there is county data, get the county geometries and combine them\n",
    "                fips_tmp = get_fips(l[0])\n",
    "                geom_data_tmp = gpd.GeoSeries(gdf_counties['geometry'][ (gdf_counties['STATEFP'] == fips_tmp) & (gdf_counties['NAMELSAD'] == l[1]) ].values)\n",
    "                if geom_data_tmp.empty:\n",
    "                    continue\n",
    "                if geom_data_tmp.apply(lambda geom: isinstance(geom, MultiPolygon)).all():\n",
    "#                     print('----> encountering multipolygon for {}, skipping'.format(l[0]))\n",
    "                    all_geometries.extend([polygon for polygon in geom_data_tmp.geometry[0].geoms])\n",
    "                else:\n",
    "                    all_geometries.extend( [Polygon(geom_data_tmp.geometry[0])] )\n",
    "\n",
    "    combined_geometry = gpd.GeoSeries(all_geometries).unary_union\n",
    "\n",
    "    return combined_geometry, row_states_counties\n",
    "\n",
    "def get_eaglei_geometries(row_county,row_state,gdf_states,gdf_counties):\n",
    "    '''\n",
    "        inputs: \n",
    "            - row of eaglei data for county (eagelei['county'].iloc[x])\n",
    "            - row of eaglei data for state (eagelei['state'].iloc[x])\n",
    "            - data for states geometries\n",
    "            - data for county geometries\n",
    "        dependencies:\n",
    "            - extract_state_county_pairs function\n",
    "    '''\n",
    "    \n",
    "    all_geometries = []\n",
    "    fips_tmp = get_fips(row_state)\n",
    "    geom_data_tmp = gpd.GeoSeries(gdf_counties['geometry'][ (gdf_counties['STATEFP'] == fips_tmp) & (gdf_counties['NAMELSAD'] == row_county + ' County') ].values)\n",
    "    if geom_data_tmp.empty:\n",
    "        return np.nan\n",
    "    if geom_data_tmp.apply(lambda geom: isinstance(geom, MultiPolygon)).all():\n",
    "        all_geometries.extend([polygon for polygon in geom_data_tmp.geometry[0].geoms])\n",
    "    else:\n",
    "        all_geometries.extend( [Polygon(geom_data_tmp.geometry[0])] )\n",
    "    \n",
    "\n",
    "    combined_geometry = gpd.GeoSeries(all_geometries).unary_union\n",
    "\n",
    "    return combined_geometry\n",
    "\n",
    "# Dictionary mapping state abbreviations to FIPS codes\n",
    "# List of tuples containing (state_abbreviation, fips_code, full_state_name)\n",
    "state_fips = [\n",
    "    ('AL', '01', 'Alabama'),\n",
    "    ('AK', '02', 'Alaska'),\n",
    "    ('AZ', '04', 'Arizona'),\n",
    "    ('AR', '05', 'Arkansas'),\n",
    "    ('CA', '06', 'California'),\n",
    "    ('CO', '08', 'Colorado'),\n",
    "    ('CT', '09', 'Connecticut'),\n",
    "    ('DE', '10', 'Delaware'),\n",
    "    ('FL', '12', 'Florida'),\n",
    "    ('GA', '13', 'Georgia'),\n",
    "    ('HI', '15', 'Hawaii'),\n",
    "    ('ID', '16', 'Idaho'),\n",
    "    ('IL', '17', 'Illinois'),\n",
    "    ('IN', '18', 'Indiana'),\n",
    "    ('IA', '19', 'Iowa'),\n",
    "    ('KS', '20', 'Kansas'),\n",
    "    ('KY', '21', 'Kentucky'),\n",
    "    ('LA', '22', 'Louisiana'),\n",
    "    ('ME', '23', 'Maine'),\n",
    "    ('MD', '24', 'Maryland'),\n",
    "    ('MA', '25', 'Massachusetts'),\n",
    "    ('MI', '26', 'Michigan'),\n",
    "    ('MN', '27', 'Minnesota'),\n",
    "    ('MS', '28', 'Mississippi'),\n",
    "    ('MO', '29', 'Missouri'),\n",
    "    ('MT', '30', 'Montana'),\n",
    "    ('NE', '31', 'Nebraska'),\n",
    "    ('NV', '32', 'Nevada'),\n",
    "    ('NH', '33', 'New Hampshire'),\n",
    "    ('NJ', '34', 'New Jersey'),\n",
    "    ('NM', '35', 'New Mexico'),\n",
    "    ('NY', '36', 'New York'),\n",
    "    ('NC', '37', 'North Carolina'),\n",
    "    ('ND', '38', 'North Dakota'),\n",
    "    ('OH', '39', 'Ohio'),\n",
    "    ('OK', '40', 'Oklahoma'),\n",
    "    ('OR', '41', 'Oregon'),\n",
    "    ('PA', '42', 'Pennsylvania'),\n",
    "    ('RI', '44', 'Rhode Island'),\n",
    "    ('SC', '45', 'South Carolina'),\n",
    "    ('SD', '46', 'South Dakota'),\n",
    "    ('TN', '47', 'Tennessee'),\n",
    "    ('TX', '48', 'Texas'),\n",
    "    ('UT', '49', 'Utah'),\n",
    "    ('VT', '50', 'Vermont'),\n",
    "    ('VA', '51', 'Virginia'),\n",
    "    ('WA', '53', 'Washington'),\n",
    "    ('WV', '54', 'West Virginia'),\n",
    "    ('WI', '55', 'Wisconsin'),\n",
    "    ('WY', '56', 'Wyoming'),\n",
    "]\n",
    "\n",
    "def get_fips(state_name):\n",
    "    for abbr, fips, name in state_fips:\n",
    "        if name.upper() == state_name.upper():\n",
    "            return fips\n",
    "    return None, None  # Return None if not found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3164cf2",
   "metadata": {},
   "source": [
    "##### Country/State location data\n",
    "Geting the data from [US Census Bureau Tiger platform](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75908b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_states = gpd.read_file('/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/location_data/Census_Bureau_Data/tl_2023_us_state/tl_2023_us_state.shp')\n",
    "gdf_counties = gpd.read_file('/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/location_data/Census_Bureau_Data/tl_2023_us_county/tl_2023_us_county.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195fd2d-c896-4c9f-aa11-5c4b08308c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of the states data is: {:.2f} MB'.format(sys.getsizeof(gdf_states) / 10**6))\n",
    "print('size of the counties data is: {:.2f} MB'.format(sys.getsizeof(gdf_counties) / 10**6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb18a2f-a3e1-4a59-85e8-57119759c8a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge into one outage data file as a gpd\n",
    "outage_columns = ['outage_start_time', 'outage_stop_time', 'customers_affected','geometry','source']\n",
    "outage_data_gdf = gpd.GeoDataFrame(pd.DataFrame(columns=outage_columns), geometry='geometry')\n",
    "\n",
    "outage_data_gdf.set_crs(\"EPSG:4326\", inplace=True)  # WGS84 - lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e33b1-602f-45d1-b4ff-405b8a3d601b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# growing the outage data with oe417 data\n",
    "\n",
    "for o in range(len(oe417)):\n",
    "    \n",
    "    # get start and end times\n",
    "    str_date_start = oe417['Date Event Began'].iloc[o]\n",
    "    dt_time_start = oe417['Time Event Began'].iloc[o]\n",
    "    str_date_end = oe417['Date of Restoration'].iloc[o]\n",
    "    dt_time_end = oe417['Time of Restoration'].iloc[o]\n",
    "    try:\n",
    "        dt_start = get_datetime_oe417(str_date_start,dt_time_start)\n",
    "    except:\n",
    "        dt_start = np.nan\n",
    "    try:\n",
    "        dt_end = get_datetime_oe417(str_date_end,dt_time_end)\n",
    "    except:\n",
    "        dt_end = np.nan    \n",
    "\n",
    "    # get the geometries of the areas affected\n",
    "    combined_geometry, states_counties = get_oe417_geometries(oe417['Area Affected'].iloc[o],gdf_states,gdf_counties)\n",
    "    \n",
    "    new_row = {\n",
    "        'source': 'oe417', \n",
    "        'area affected': states_counties, \n",
    "        'outage_start_time': dt_start,\n",
    "        'outage_stop_time': dt_end,\n",
    "        'customers_affected': oe417['Number of Customers Affected'].iloc[o],\n",
    "        'geometry': combined_geometry\n",
    "    }\n",
    "    \n",
    "    outage_data_gdf = pd.concat([outage_data_gdf, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e484e-e53b-4b00-982a-616a4d16cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outage_data_gdf#['outage_start_time'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd2bd8-1457-4193-a03c-1d130722dc4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# growing the outage data with eaglei data\n",
    "\n",
    "for e in range(len(eaglei)):\n",
    "    \n",
    "    # get start times\n",
    "    try:\n",
    "        dt_start = pd.to_datetime(eaglei['run_start_time'].iloc[e], format='%m/%d/%y %H:%M')\n",
    "    except:\n",
    "        dt_start = np.nan\n",
    "    \n",
    "    combined_geometry = get_eaglei_geometries(eaglei['county'].iloc[e],eaglei['state'].iloc[e],gdf_states,gdf_counties)\n",
    "    \n",
    "    outage_data_gdf = outage_data_gdf.append({\n",
    "                                                'source': 'eaglei', \n",
    "                                                'area affected': [(eaglei['state'].iloc[e],eaglei['county'].iloc[e]+' County')], \n",
    "                                                'outage_start_time': dt_start,\n",
    "                                                'outage_stop_time': np.nan,\n",
    "                                                'customers_affected': eaglei['customers_out'].iloc[e],\n",
    "                                                'geometry': combined_geometry\n",
    "                                                    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34084ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23979c96-2a20-44eb-8ad7-37b47e46c09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c60a2ee2",
   "metadata": {},
   "source": [
    "#### Candidate search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c35991",
   "metadata": {},
   "source": [
    "#### first group the modis wildfire data by datetime and nearby lat-long points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756113d-9c46-4964-8f45-0d7557af628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "\n",
    "def aggregate_points_within_radius(gdf, radius=0.01):\n",
    "    \"\"\"\n",
    "    Aggregates points by datetime and spatial proximity into polygons.\n",
    "\n",
    "    Parameters:\n",
    "    - df: GeoDataFrame with columns [\"datetime\", \"geometry\"]\n",
    "    - radius: Proximity radius in degrees (~0.01 degrees â‰ˆ 1.1 km) for grouping\n",
    "    \n",
    "    Returns:\n",
    "    - A new GeoDataFrame with datetime and polygon geometry\n",
    "    \"\"\"\n",
    "    # Group by datetime\n",
    "    grouped = gdf.groupby('datetime')\n",
    "\n",
    "    result_rows = []\n",
    "\n",
    "    for datetime_value, group in grouped:\n",
    "        points = group['geometry'].tolist()\n",
    "        clustered_points = []\n",
    "\n",
    "        # Track visited points\n",
    "        visited = set()\n",
    "\n",
    "        for i, point in enumerate(points):\n",
    "            if i in visited:\n",
    "                continue\n",
    "\n",
    "            # Start a cluster\n",
    "            cluster = [point]\n",
    "            visited.add(i)\n",
    "\n",
    "            for j, other_point in enumerate(points):\n",
    "                if j not in visited and point.distance(other_point) <= radius:\n",
    "                    cluster.append(other_point)\n",
    "                    visited.add(j)\n",
    "\n",
    "            # Create a polygon from the clustered points\n",
    "            polygon = MultiPoint(cluster).convex_hull\n",
    "            result_rows.append({\n",
    "                'datetime': datetime_value,\n",
    "                'geometry': polygon\n",
    "            })\n",
    "\n",
    "    # Create a new GeoDataFrame from the result\n",
    "    result_gdf = gpd.GeoDataFrame(result_rows, columns=['datetime', 'geometry'], crs=gdf.crs)\n",
    "    return result_gdf\n",
    "\n",
    "\n",
    "# Example usage with your GeoDataFrame\n",
    "gdf_modis_wildfire_result = aggregate_points_within_radius(gdf_modis_wildfire, radius=0.01)\n",
    "print(gdf_modis_wildfire_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec10ab",
   "metadata": {},
   "source": [
    "##### Optionally plot grouped wildfire instances in MODIS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff8e86-eae1-42e8-896c-7981340f2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 0.1  # Example radius in degrees (adjust based on your CRS)\n",
    "\n",
    "candidates = []\n",
    "# candidate_columns = ['outage_start_time', 'outage_stop_time', 'customers_affected','geometry','source']\n",
    "# candidate_gdf = gpd.GeoDataFrame(pd.DataFrame(columns=outage_columns), geometry='geometry')\n",
    "\n",
    "\n",
    "# for w in range(len(gdf_modis_wildfire)):\n",
    "for index_w, row_w in gdf_modis_wildfire_result.iterrows():\n",
    "    \n",
    "    # Identify overlap in time with the power grid disturbance\n",
    "    wildfire_datetime = row_w.datetime #gdf_modis_wildfire['datetime'].iloc[w]\n",
    "    filtered_outage_data_gdf = outage_data_gdf[\n",
    "                                            (outage_data_gdf['outage_start_time'] >= wildfire_datetime - pd.Timedelta(hours=1)) &\n",
    "                                            (outage_data_gdf['outage_start_time'] <= wildfire_datetime + pd.Timedelta(days=1))\n",
    "                                            ]\n",
    "    \n",
    "    # Identify overlap in space with the power grid disturbance\n",
    "    geom = row_w.geometry\n",
    "    \n",
    "    # Check if the geometry is a Point\n",
    "    if geom.geom_type == 'Point':\n",
    "        #   Create a buffer (circular area) around the point\n",
    "        wildfire_point = gdf_modis_wildfire['geometry'].iloc[row_w.name]\n",
    "        buffered_wildfire_area = geom.buffer(radius)\n",
    "        disp_geom = geom.coords[0]\n",
    "\n",
    "    # Check if the geometry is a Polygon\n",
    "    elif geom.geom_type == 'Polygon':\n",
    "        buffered_wildfire_area = geom\n",
    "        disp_geom = geom.exterior.coords[0]\n",
    "\n",
    "    \n",
    "    for f in range(len(filtered_outage_data_gdf)):\n",
    "        if buffered_wildfire_area.intersects(filtered_outage_data_gdf['geometry'].iloc[f]):\n",
    "            \n",
    "            # Check space weather data for space weather conditions (if at any time in the surrounding 24 hours AE exceeded 200 nT)\n",
    "            sm_data = get_sm_one_minute((wildfire_datetime - pd.Timedelta(hours=24)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                                        (wildfire_datetime + pd.Timedelta(hours=12)).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            \n",
    "            if sm_data['SME'][sm_data['SME']>200.].any():\n",
    "\n",
    "\n",
    "                # save as a candidate\n",
    "                print('candidate at {}! \\n wildfire: \\n \\t {} \\n outage: \\n \\t {} for {} \\n sp wx: \\n \\t {} at {}'.format(wildfire_datetime,\n",
    "                                                                                                                             disp_geom,\n",
    "                                                                                                                             filtered_outage_data_gdf['outage_start_time'].iloc[f],\n",
    "                                                                                                                             filtered_outage_data_gdf['area affected'].iloc[f],\n",
    "                                                                                                                             sm_data['SME'][sm_data['SME']==sm_data['SME'].max()].values,\n",
    "                                                                                                                             sm_data.index[sm_data['SME']==sm_data['SME'].max()].values,))\n",
    "                candidates.append({\n",
    "                                    'wildfire datetime': wildfire_datetime,\n",
    "                                    'wildfire geometry': geom,\n",
    "                                    'outage_start_time': filtered_outage_data_gdf['outage_start_time'].iloc[f],\n",
    "                                    'outage_stop_time': filtered_outage_data_gdf['outage_stop_time'].iloc[f],\n",
    "                                    'customers affected': filtered_outage_data_gdf['customers_affected'].iloc[f],\n",
    "                                    'outage geometry': filtered_outage_data_gdf['geometry'].iloc[f],\n",
    "                                    'area affected': filtered_outage_data_gdf['area affected'].iloc[f],\n",
    "                                    'max SME': sm_data['SME'][sm_data['SME']==sm_data['SME'].max()].values,\n",
    "\n",
    "                                    })\n",
    "                break\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671b3bb2-c88d-4620-bd26-64eb0a2cf8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c883511",
   "metadata": {},
   "source": [
    "#### Candidate search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaea6bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "radius = 0.1  # Example radius in degrees (adjust based on your CRS)\n",
    "\n",
    "candidates = []\n",
    "# candidate_columns = ['outage_start_time', 'outage_stop_time', 'customers_affected','geometry','source']\n",
    "# candidate_gdf = gpd.GeoDataFrame(pd.DataFrame(columns=outage_columns), geometry='geometry')\n",
    "\n",
    "\n",
    "# for w in range(len(gdf_modis_wildfire)):\n",
    "for index_w, row_w in gdf_modis_wildfire_result.iterrows():\n",
    "    \n",
    "    # Identify overlap in time with the power grid disturbance\n",
    "    wildfire_datetime = row_w.datetime #gdf_modis_wildfire['datetime'].iloc[w]\n",
    "    filtered_outage_data_gdf = outage_data_gdf[\n",
    "                                            (outage_data_gdf['outage_start_time'] >= wildfire_datetime - pd.Timedelta(hours=1)) &\n",
    "                                            (outage_data_gdf['outage_start_time'] <= wildfire_datetime + pd.Timedelta(days=1))\n",
    "                                            ]\n",
    "    \n",
    "    # Identify overlap in space with the power grid disturbance\n",
    "    geom = row_w.geometry\n",
    "    \n",
    "    # Check if the geometry is a Point\n",
    "    if geom.geom_type == 'Point':\n",
    "        #   Create a buffer (circular area) around the point\n",
    "        wildfire_point = gdf_modis_wildfire['geometry'].iloc[row_w.name]\n",
    "        buffered_wildfire_area = geom.buffer(radius)\n",
    "        disp_geom = geom.coords[0]\n",
    "\n",
    "    # Check if the geometry is a Polygon\n",
    "    elif geom.geom_type == 'Polygon':\n",
    "        buffered_wildfire_area = geom\n",
    "        disp_geom = geom.exterior.coords[0]\n",
    "\n",
    "    \n",
    "    for f in range(len(filtered_outage_data_gdf)):\n",
    "        if buffered_wildfire_area.intersects(filtered_outage_data_gdf['geometry'].iloc[f]):\n",
    "            \n",
    "            # Check space weather data for space weather conditions (if at any time in the surrounding 24 hours AE exceeded 200 nT)\n",
    "            sm_data = get_sm_one_minute((wildfire_datetime - pd.Timedelta(hours=24)).strftime('%Y-%-m-%-d %H:%M:%S'),\n",
    "                                        (wildfire_datetime + pd.Timedelta(hours=12)).strftime('%Y-%-m-%-d %H:%M:%S'))\n",
    "            \n",
    "            if sm_data['SME'][sm_data['SME']>200.].any():\n",
    "\n",
    "\n",
    "                # save as a candidate\n",
    "                print('candidate at {}! \\n wildfire: \\n \\t {} \\n outage: \\n \\t {} for {} \\n sp wx: \\n \\t {} at {}'.format(wildfire_datetime,\n",
    "                                                                                                                             disp_geom,\n",
    "                                                                                                                             filtered_outage_data_gdf['outage_start_time'].iloc[f],\n",
    "                                                                                                                             filtered_outage_data_gdf['area affected'].iloc[f],\n",
    "                                                                                                                             sm_data['SME'][sm_data['SME']==sm_data['SME'].max()].values,\n",
    "                                                                                                                             sm_data.index[sm_data['SME']==sm_data['SME'].max()].values,))\n",
    "                candidates.append({\n",
    "                                    'wildfire datetime': wildfire_datetime,\n",
    "                                    'wildfire geometry': geom,\n",
    "                                    'outage_start_time': filtered_outage_data_gdf['outage_start_time'].iloc[f],\n",
    "                                    'outage_stop_time': filtered_outage_data_gdf['outage_stop_time'].iloc[f],\n",
    "                                    'customers affected': filtered_outage_data_gdf['customers_affected'].iloc[f],\n",
    "                                    'outage geometry': filtered_outage_data_gdf['geometry'].iloc[f],\n",
    "                                    'area affected': filtered_outage_data_gdf['area affected'].iloc[f],\n",
    "                                    'max SME': sm_data['SME'][sm_data['SME']==sm_data['SME'].max()].values,\n",
    "\n",
    "                                    })\n",
    "                break\n",
    "    continue\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70422f-55a6-45bf-a7aa-958a20225f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5698971-a0dc-4423-89fd-7236d5952657",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414c41e-40b5-4b27-a4d5-0e4253b80539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39c565-745a-448e-bd87-2e2807c91786",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_candidates = gpd.GeoDataFrame(candidates)\n",
    "gdf_candidates = gdf_candidates.set_geometry(\"outage geometry\")\n",
    "\n",
    "\n",
    "# Save with pickle \n",
    "gdf_candidates.to_pickle(\"/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/candidate_multihazards_data/wildfire_space-weather_2023_candidates.pkl\")\n",
    "# loaded_gdf = gpd.read_pickle(\"hazard_events.pkl\")\n",
    "\n",
    "# # Save to GeoJSON (need to ensure all geometry types are the same before this works)\n",
    "# gdf_candidates.to_file(\"/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/candidate_multihazards_data/wildfire_space-weather_2023_candidates.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93714b0-46a6-499f-be30-d975cbc2a5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b4dc9a6-0feb-45c8-85ce-697bec302703",
   "metadata": {},
   "source": [
    "#### Load EM-DAT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39aab3-0a3e-4820-88d3-a8c49fd743cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_emdat = '/Users/ryanmc/Documents/Conferences/Jack_Eddy_Symposium_2022/dev/emdat_data/'\n",
    "file_emdat = 'public_emdat_2023_US.xlsx'\n",
    "df_emdat = pd.read_excel(os.path.join(directory_emdat,file_emdat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f9165-34f3-4157-b28d-ad61f6662f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_emdat.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf016c-6c9f-4e7a-a2c4-d631d6a95481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_emdat[['Start Year', 'Start Month', 'Start Day']]\n",
    "df_emdat[['End Year', 'End Month', 'End Day']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eff5d6-f145-4637-9ab2-4f45a7b359de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3257b-f0c2-4a1f-b147-74f247689132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emdat['start_datetime'] = df_emdat.apply(\n",
    "    lambda row: pd.Timestamp(int(row['Start Year']), int(row['Start Month']), int(row['Start Day'])) \n",
    "    if pd.notna(row['Start Year']) and pd.notna(row['Start Month']) and pd.notna(row['Start Day'])\n",
    "    else pd.NaT,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_emdat['end_datetime'] = df_emdat.apply(\n",
    "    lambda row: pd.Timestamp(int(row['End Year']), int(row['End Month']), int(row['End Day'])) \n",
    "    if pd.notna(row['End Year']) and pd.notna(row['End Month']) and pd.notna(row['End Day'])\n",
    "    else pd.NaT,\n",
    "    axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc50fca-474c-4432-af89-036a85364842",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_candidates.columns.tolist() #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d2210-260a-416f-9dbd-c33704c4e963",
   "metadata": {},
   "source": [
    "#### Compare potential events across candidates and database(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcec637-a5d3-4a4c-8550-6a60ed433c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx,row_emdat in df_emdat.iterrows():\n",
    "    tmp_diff = (gdf_candidates['outage_start_time'] - row_emdat['start_datetime']).abs()\n",
    "    result = gdf_candidates[tmp_diff <= pd.Timedelta(days=1)]\n",
    "\n",
    "    print('overlap of time around {}, \\n \\t location candidate = {} \\n \\t  location emdat = {}'.format(row_emdat['start_datetime'],result['area affected'],row_emdat['Location']))\n",
    "    print('\\n\\n')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f7046-9152-4e4a-867d-052763bad00b",
   "metadata": {},
   "source": [
    "Check on\n",
    "\n",
    "\n",
    "overlap of time around 2023-04-01 00:00:00, \n",
    " \t location candidate = 10    [(Arkansas, None)]\n",
    "11    [(Illinois, None)]\n",
    "Name: area affected, dtype: object \n",
    " \t  location emdat = Texas, Louisiana, Oklahoma, Kansas, Illinois, Missouri, Nebraska, Washington, Oregon, Montana\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4347a6-65e9-4382-830d-b1fe5e8cfddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_candidates_multihazard = gdf_candidates[(gdf_candidates['outage_start_time'] >= datetime(2023,3,31)) & (gdf_candidates['outage_start_time'] <= datetime(2023,4,2))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76814c2-a48e-47c2-b181-6adf7e0fcc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import folium\n",
    "# import time\n",
    "# from shapely.geometry import Point, Polygon\n",
    "# from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "# Create a Folium map centered over CONUS\n",
    "conus_map = folium.Map(location=[37.5, -95], zoom_start=5)\n",
    "\n",
    "for index, row in gdf_candidates_multihazard.iterrows():\n",
    "    geom = row['wildfire geometry']\n",
    "\n",
    "    # Check if the geometry is a Point\n",
    "    if geom.geom_type == 'Point':\n",
    "        marker_layer = folium.Marker(\n",
    "            location=[geom.y, geom.x],\n",
    "            popup=\"Point Geometry\"\n",
    "        ).add_to(conus_map)\n",
    "    \n",
    "    # Check if the geometry is a Polygon\n",
    "    elif geom.geom_type == 'Polygon':\n",
    "        poly_layer = folium.Polygon(\n",
    "            locations=[(y, x) for x, y in geom.exterior.coords],\n",
    "            color=\"blue\",\n",
    "            fill=True,\n",
    "            fill_opacity=0.4,\n",
    "            popup=\"Polygon Geometry\"\n",
    "        ).add_to(conus_map)\n",
    "\n",
    "display(conus_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075f430-6831-4cbc-a0ce-a4c6c38ea028",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in gdf_candidates_multihazard.iterrows():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f54b6-c7ed-45a0-91c4-4e0450bb3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row['outage geometry']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26686fe0-e278-4f05-aeb0-13f22b5c2978",
   "metadata": {},
   "source": [
    "TODO\n",
    "- Compile the concrete information that we know about this event\n",
    "- Determine what continuous data we have (imagery over time, space weather data (GIC and SuperMAG)\n",
    "- Can these 'layers' (need the weather and space weather spatiotemporal maps) yield network data and how might I explore its information over time?\n",
    "- get additional data - Geo-electric field\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50d7c0-f7c4-4c06-977a-18e58b90f684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spwxr_network_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
